{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103fe49d",
   "metadata": {},
   "source": [
    "# Nouakchott, Mauritania City Scan Data Cleaning\n",
    "##### Dicembre 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3442e1",
   "metadata": {},
   "source": [
    "Basic data cleaning pipeline for appropriate CSV preparation necessary for City Scan JavaScript plots for Nouakchott, Mauritania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154863ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory changes\n",
      "current working directory is: /Users/carolinecullinan/dev/wb/nouakchott-mauritania\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# add project root to python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# change to project root directory\n",
    "os.chdir('../')\n",
    "print(\"directory changes\")\n",
    "print(f\"current working directory is:\", os.getcwd())\n",
    "\n",
    "# local imports (after changing directory)\n",
    "from src.clean import clean_pg, clean_pas, clean_rwi_area, clean_uba, clean_uba_area, clean_lc, clean_pug, clean_pv, clean_pv_area, clean_aq_area, clean_summer_area, clean_ndvi_area, clean_deforestation_area, clean_flood, clean_e, clean_s, clean_ls_area, clean_ee, clean_l_area, clean_fwi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e3513",
   "metadata": {},
   "source": [
    "# POPULATION AND DEMOGRAPHIC TRENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097a900d",
   "metadata": {},
   "source": [
    "### pg.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_pga\" / \"chart_pga\" ; and\n",
    "#### 2.) \"plot_pgp\" / \"chart_pg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d3b286",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw/population-growth.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# POPULATION & DEMOGRAPHIC TRENDS - pg.csv preparation for Observable Notebook plot functions/charts:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 1.) \"plot_pga\" / \"chart_pga\" (absolute population growth); and \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 2.) \"plot_pgp\" / \"chart_pgp\" (population growth percentage)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# load \"raw\" (i.e. \"dirty\") tabular output data\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m raw_df_pg \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/raw/population-growth.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# updatefile path\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# basic info about raw data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw population growth data info:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw/population-growth.csv'"
     ]
    }
   ],
   "source": [
    "# POPULATION & DEMOGRAPHIC TRENDS - pg.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_pga\" / \"chart_pga\" (absolute population growth); and \n",
    "# 2.) \"plot_pgp\" / \"chart_pgp\" (population growth percentage)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_pg = pd.read_csv('data/raw/population-growth.csv') # updatefile path\n",
    "\n",
    "# basic info about raw data\n",
    "print(\"Raw population growth data info:\")\n",
    "print(f\"Shape: {raw_df_pg.shape}\")\n",
    "print(f\"Columns: {list(raw_df_pg.columns)}\")\n",
    "print(f\"Date range: {raw_df_pg['Year'].min()} - {raw_df_pg['Year'].max()}\")\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_pg.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean data using clean_pg function in clean.py\n",
    "try:\n",
    "    cleaned_df_pg = clean_pg('data/raw/population-growth.csv') # updatefile path\n",
    "    print(\"Population growth data cleaned successfully!\")\n",
    "    \n",
    "    # cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_pg.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_pg.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_pg.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_pg.isnull().sum().sum()}\")\n",
    "    print(f\"- Year range: {cleaned_df_pg['yearName'].min()} - {cleaned_df_pg['yearName'].max()}\")\n",
    "    print(f\"- Population range: {cleaned_df_pg['population'].min():,} - {cleaned_df_pg['population'].max():,}\")\n",
    "    print(f\"- Growth rate range: {cleaned_df_pg['populationGrowthPercentage'].min():.3f}% - {cleaned_df_pg['populationGrowthPercentage'].max():.3f}%\")\n",
    "    \n",
    "    # check for any potential data quality issues\n",
    "    if cleaned_df_pg['populationGrowthPercentage'].isna().sum() > 0:\n",
    "        print(f\"Note: {cleaned_df_pg['populationGrowthPercentage'].isna().sum()} missing growth rate values (expected for first year)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error cleaning population growth data: {e}\")\n",
    "    print(\"Check that 'data/raw/population-growth.csv' exists and has the correct format\")\n",
    "\n",
    "# save cleaned data as csv file - pg.csv, and export\n",
    "# (this is handled automatically by clean_pg function, but confirming)\n",
    "if 'cleaned_df_pg' in locals():\n",
    "    print(f\"Cleaned data saved to: data/processed/pg.csv\")\n",
    "else:\n",
    "    print(\"No cleaned data available to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b4d33a",
   "metadata": {},
   "source": [
    "### pas.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_pas\" / \"chart_pas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391cf0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw population age structure data info:\n",
      "Shape: (36, 3)\n",
      "Columns: ['age_group', 'sex', 'population']\n",
      "Age groups: ['0-1', '1-4', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39', '40-44', '45-49', '5-9', '50-54', '55-59', '60-64', '65-69', '70-74', '75-79', '80+']\n",
      "Sex categories: ['f' 'm']\n",
      "Total population: 1,345,253\n",
      "Data preview:\n",
      "  age_group sex    population\n",
      "0       1-4   f  62353.000043\n",
      "1       1-4   m  64829.461383\n",
      "2       0-1   f  18782.104672\n",
      "3       0-1   m  19528.068560\n",
      "4       5-9   f  73314.980869\n",
      "\n",
      "==================================================\n",
      "\n",
      "Warning: Could not sort by age bracket (\"['age_sort'] not found in axis\"). Using default sorting.\n",
      "Cleaned data saved to: data/processed/pas.csv\n",
      "Total population: 1,345,253\n",
      "Age brackets: 17\n",
      "Sex categories: 2\n",
      "Total records: 34\n",
      " Population age structure data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (34, 5)\n",
      "Cleaned data columns: ['ageBracket', 'sex', 'count', 'percentage', 'yearName']\n",
      "Sample of cleaned data:\n",
      "  ageBracket     sex     count  percentage  yearName\n",
      "0        0-4  female  81135.10    6.031215      2021\n",
      "1        0-4    male  84357.53    6.270755      2021\n",
      "2      10-14  female  61525.84    4.573551      2021\n",
      "3      10-14    male  55590.87    4.132373      2021\n",
      "4      15-19  female  70229.90    5.220571      2021\n",
      "5      15-19    male  63455.31    4.716979      2021\n",
      "6      20-24  female  81528.87    6.060486      2021\n",
      "7      20-24    male  78565.25    5.840183      2021\n",
      "8      25-29  female  75708.04    5.627792      2021\n",
      "9      25-29    male  72956.00    5.423218      2021\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 0\n",
      "- Age brackets: ['0-4', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39', '40-44', '45-49', '5-9', '50-54', '55-59', '60-64', '65-69', '70-74', '75-79', '80+']\n",
      "- Sex categories: ['female', 'male']\n",
      "- Population count range: 968 - 84,358\n",
      "- Percentage range: 0.072% - 6.271%\n",
      "- Year: 2021\n",
      "- Total percentage sum: 100.000% (should be ~100%)\n",
      "- Population by sex: Female: 688,580, Male: 656,673\n",
      "- Age brackets: 17, Total records: 34\n",
      "Cleaned data saved to: data/processed/pas.csv\n",
      "\n",
      "Data structure summary:\n",
      "- Columns: ['ageBracket', 'sex', 'count', 'percentage', 'yearName']\n",
      "- Records per sex: 17, 17\n",
      "- Data types: {'ageBracket': dtype('O'), 'sex': dtype('O'), 'count': dtype('float64'), 'percentage': dtype('float64'), 'yearName': dtype('int64')}\n"
     ]
    }
   ],
   "source": [
    "# POPULATION AGE SEX - pas.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_pas\" / \"chart_pas\" (population age sex, i.e., population by sex and age bracket, (i.e., Population Distribution by Age & Sex, xxxx))\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_pas = pd.read_csv('data/raw/2025-11-mauritania-nouakchott_02-process-output_tabular_nouakchott_demographics.csv') # updatefile path\n",
    "\n",
    "# basic info about raw data\n",
    "print(\"Raw population age structure data info:\")\n",
    "print(f\"Shape: {raw_df_pas.shape}\")\n",
    "print(f\"Columns: {list(raw_df_pas.columns)}\")\n",
    "print(f\"Age groups: {sorted(raw_df_pas['age_group'].unique())}\")\n",
    "print(f\"Sex categories: {raw_df_pas['sex'].unique()}\")\n",
    "print(f\"Total population: {raw_df_pas['population'].sum():,.0f}\")\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_pas.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean data using clean_pas function in clean.py\n",
    "try:\n",
    "    cleaned_df_pas = clean_pas('data/raw/2025-11-mauritania-nouakchott_02-process-output_tabular_nouakchott_demographics.csv') # updatefile path\n",
    "    print(\" Population age structure data cleaned successfully!\")\n",
    "    \n",
    "    # cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_pas.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_pas.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_pas.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_pas.isnull().sum().sum()}\")\n",
    "    print(f\"- Age brackets: {sorted(cleaned_df_pas['ageBracket'].unique())}\")\n",
    "    print(f\"- Sex categories: {sorted(cleaned_df_pas['sex'].unique())}\")\n",
    "    print(f\"- Population count range: {cleaned_df_pas['count'].min():,.0f} - {cleaned_df_pas['count'].max():,.0f}\")\n",
    "    print(f\"- Percentage range: {cleaned_df_pas['percentage'].min():.3f}% - {cleaned_df_pas['percentage'].max():.3f}%\")\n",
    "    print(f\"- Year: {cleaned_df_pas['yearName'].iloc[0]}\")\n",
    "    \n",
    "    # data quality checks\n",
    "    total_percentage = cleaned_df_pas['percentage'].sum()\n",
    "    print(f\"- Total percentage sum: {total_percentage:.3f}% (should be ~100%)\")\n",
    "    \n",
    "    if abs(total_percentage - 100) > 0.1:\n",
    "        print(f\"Warning: Percentage sum deviates from 100% by {abs(total_percentage - 100):.3f}%\")\n",
    "    \n",
    "    # check for balanced sex representation\n",
    "    sex_counts = cleaned_df_pas.groupby('sex')['count'].sum()\n",
    "    print(f\"- Population by sex: Female: {sex_counts.get('female', 0):,.0f}, Male: {sex_counts.get('male', 0):,.0f}\")\n",
    "    \n",
    "    # check age bracket coverage\n",
    "    expected_brackets = len(cleaned_df_pas['ageBracket'].unique())\n",
    "    actual_records = len(cleaned_df_pas)\n",
    "    print(f\"- Age brackets: {expected_brackets}, Total records: {actual_records}\")\n",
    "    \n",
    "    if actual_records != expected_brackets * 2:  # should be 2 records per age bracket (male/female)\n",
    "        print(f\"Note: Expected {expected_brackets * 2} records (2 per age bracket), found {actual_records}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error cleaning population age structure data: {e}\")\n",
    "    print(\"Check that the demographics CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: age_group, sex, population\")\n",
    "\n",
    "# save cleaned data as csv file - pas.csv, and export\n",
    "# (this is handled automatically by clean_pas function, but confirming)\n",
    "if 'cleaned_df_pas' in locals():\n",
    "    print(f\"Cleaned data saved to: data/processed/pas.csv\")\n",
    "    \n",
    "    # preview data structure\n",
    "    print(f\"\\nData structure summary:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_pas.columns)}\")\n",
    "    print(f\"- Records per sex: {len(cleaned_df_pas[cleaned_df_pas['sex'] == 'female'])}, {len(cleaned_df_pas[cleaned_df_pas['sex'] == 'male'])}\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_pas.dtypes)}\")\n",
    "else:\n",
    "    print(\"No cleaned data available to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c9737",
   "metadata": {},
   "source": [
    "### RELATIVE WEALTH INDEX (RWI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f314f",
   "metadata": {},
   "source": [
    "### rwi_area.csv preparation (i.e., Percentage of area with different relative wealth index levels - \"Least wealthy\", \"Less wealthy\", \"Average wealth\", \"More wealthy\", \"Most wealthy\")\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_rwi_area\" / \"chart_rwi_area\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a24962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELATIVE WEALTH INDEX (RWI) - rwi_area.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_rwi_area\" / \"chart_rwi_area\" (i.e., Percentage of area with different relative wealth index levels - \"Least wealthy\", \"Less wealthy\", \"Average wealth\", \"More wealthy\", \"Most wealthy\")\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "input_gpkg_path = 'data/raw/2025-11-mauritania-nouakchott_02-process-output_spatial_nouakchott_rwi.gpkg'  # updatefile path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RELATIVE WEALTH INDEX DATA PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Analyzing GeoPackage data structure...\")\n",
    "\n",
    "try:\n",
    "    gdf = gpd.read_file(input_gpkg_path)\n",
    "    \n",
    "    print(f\"Columns: {list(gdf.columns)}\")\n",
    "    print(f\"Total features: {len(gdf):,}\")\n",
    "    print(f\"Geometry type: {gdf.geometry.type.unique()}\")\n",
    "    print(f\"CRS: {gdf.crs}\")\n",
    "    \n",
    "    # check for 'rwi' column\n",
    "    if 'rwi' in gdf.columns:\n",
    "        rwi_data = gdf['rwi'].dropna()\n",
    "        print(f\"\\nRWI statistics:\")\n",
    "        print(f\"- Range: {rwi_data.min():.3f} to {rwi_data.max():.3f}\")\n",
    "        print(f\"- Mean: {rwi_data.mean():.3f}\")\n",
    "        print(f\"- Median: {rwi_data.median():.3f}\")\n",
    "        print(f\"- Standard deviation: {rwi_data.std():.3f}\")\n",
    "        print(f\"- Valid values: {len(rwi_data):,}\")\n",
    "        print(f\"- Null values: {gdf['rwi'].isna().sum()}\")\n",
    "    else:\n",
    "        print(f\"\\nWarning: 'rwi' column not found. Available numeric columns: {gdf.select_dtypes(include=[np.number]).columns.tolist()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not examine GeoPackage structure: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "\n",
    "# process gpkg file using clean_rwi_area function in clean.py\n",
    "try:\n",
    "    cleaned_df_rwi = clean_rwi_area(input_gpkg_path, rwi_column='rwi')\n",
    "    print(\"\\nRWI data processed successfully!\")\n",
    "    \n",
    "    # display cleaned data structure\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_rwi.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_rwi.columns)}\")\n",
    "    print(f\"\\nProcessed RWI data:\")\n",
    "    print(cleaned_df_rwi)\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData Validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_rwi.isnull().sum().sum()}\")\n",
    "    print(f\"- Wealth categories: {len(cleaned_df_rwi)}\")\n",
    "    print(f\"- Total grid cells: {cleaned_df_rwi['count'].sum():,}\")\n",
    "    print(f\"- Percentage sum: {cleaned_df_rwi['percentage'].sum():.1f}% (should be ~100%)\")\n",
    "    \n",
    "    # wealth distribution analysis\n",
    "    print(f\"\\nWealth Distribution Analysis:\")\n",
    "    \n",
    "    total_cells = cleaned_df_rwi['count'].sum()\n",
    "    \n",
    "    for idx, row in cleaned_df_rwi.iterrows():\n",
    "        print(f\"- {row['bin']}: {row['count']:,} cells ({row['percentage']:.1f}% of area)\")\n",
    "    \n",
    "    # wealth inequality metrics\n",
    "    print(f\"\\nWealth Distribution Summary:\")\n",
    "    \n",
    "    # least wealthy areas (bottom 20%)\n",
    "    least_wealthy = cleaned_df_rwi[cleaned_df_rwi['bin'].isin(['Least wealthy'])]['percentage'].sum()\n",
    "\n",
    "    # less wealthy areas (next 20%)\n",
    "    less_wealthy = cleaned_df_rwi[cleaned_df_rwi['bin'].isin(['Less wealthy'])]['percentage'].sum()\n",
    "    \n",
    "    # average wealth (middle 20%)\n",
    "    average_wealth = cleaned_df_rwi[cleaned_df_rwi['bin'] == 'Average wealth']['percentage'].sum()\n",
    "\n",
    "    # more wealthy areas (next middle/top 20%)\n",
    "    more_wealthy = cleaned_df_rwi[cleaned_df_rwi['bin'].isin(['More wealthy'])]['percentage'].sum()\n",
    "    \n",
    "    # most wealthy areas (top 20%)\n",
    "    most_wealthly = cleaned_df_rwi[cleaned_df_rwi['bin'].isin(['Most wealthy'])]['percentage'].sum()\n",
    "    \n",
    " \n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values\n",
    "    missing_values = cleaned_df_rwi.isnull().sum().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"Missing values detected: {missing_values}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for negative values in \"count\" / \"percentage\"\n",
    "    negative_counts = (cleaned_df_rwi['count'] < 0).sum()\n",
    "    negative_percentages = (cleaned_df_rwi['percentage'] < 0).sum()\n",
    "    \n",
    "    if negative_counts > 0:\n",
    "        print(f\"Negative count values: {negative_counts}\")\n",
    "        quality_issues += 1\n",
    "    if negative_percentages > 0:\n",
    "        print(f\"Negative percentage values: {negative_percentages}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check \"percentage\" sum\n",
    "    percentage_sum = cleaned_df_rwi['percentage'].sum()\n",
    "    if abs(percentage_sum - 100) > 0.1:\n",
    "        print(f\"Percentage sum deviation: {percentage_sum:.1f}% (should be ~100%)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate bins\n",
    "    duplicates = cleaned_df_rwi['bin'].duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Duplicate wealth categories: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for expected number of categories (should be 5)\n",
    "    if len(cleaned_df_rwi) != 5:\n",
    "        print(f\"Unexpected number of categories: {len(cleaned_df_rwi)} (expected 5)\")\n",
    "        print(f\"Note: This can happen if RWI values have many duplicates (especially with discrete vs continuous RWI values (e.g., 0, 1 vs. 0.0123, 0.943)) - Despite this, the percentages should always be correct because we're summing the actual grid cell areas - no data is discarded, just the bin boundaries might merge if needed.\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"No data quality issues detected\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing RWI data: {e}\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure GeoPackage file exists at the specified path\")\n",
    "    print(\"   2. Check that the file contains 'rwi' column\")\n",
    "    print(\"   3. Verify the GeoPackage file is not corrupted\")\n",
    "    print(\"   4. Ensure geopandas library is installed: pip install geopandas\")\n",
    "    print(\"   5. Check file permissions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb23df9",
   "metadata": {},
   "source": [
    "# BUILT FORM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ce80c",
   "metadata": {},
   "source": [
    "### URBAN EXTENT AND CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d7a2bf",
   "metadata": {},
   "source": [
    "### uba.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_ubaa\" / \"chart_ubaa\" ; and\n",
    "#### 2.) \"plot_ubap\" / \"chart_ubap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea38b826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw urban built area data info:\n",
      "Shape: (31, 2)\n",
      "Columns: ['year', 'cumulative sq km']\n",
      "Year range: 1985 - 2015\n",
      "UBA range: 28.60 - 121.05 sq km\n",
      "Total data points: 31\n",
      "Data preview:\n",
      "   year  cumulative sq km\n",
      "0  1985         28.597767\n",
      "1  1986         28.597767\n",
      "2  1987         32.754955\n",
      "3  1988         36.485323\n",
      "4  1989         39.294931\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/uba.csv\n",
      "Years covered: 1985 - 2015\n",
      "Total data points: 31\n",
      "UBA range: 28.60 - 121.05 sq km\n",
      "Urban built area data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (31, 4)\n",
      "Cleaned data columns: ['year', 'yearName', 'uba', 'ubaGrowthPercentage']\n",
      "Sample of cleaned data:\n",
      "   year  yearName    uba  ubaGrowthPercentage\n",
      "0     1      1985  28.60                  NaN\n",
      "1     2      1986  28.60                0.000\n",
      "2     3      1987  32.75               14.510\n",
      "3     4      1988  36.49               11.420\n",
      "4     5      1989  39.29                7.673\n",
      "5     6      1990  45.39               15.526\n",
      "6     7      1991  45.56                0.375\n",
      "7     8      1992  45.63                0.154\n",
      "8     9      1993  45.73                0.219\n",
      "9    10      1994  47.13                3.061\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 1\n",
      "- Year range: 1985 - 2015\n",
      "- UBA range: 28.60 - 121.05 sq km\n",
      "- Growth rate range: 0.000% - 27.452%\n",
      "- Total urban expansion: 92.45 sq km over 30 years\n",
      "\n",
      "Urban growth analysis:\n",
      "- Average annual UBA growth rate: 5.123%\n",
      "Note: 1 missing growth rate values (expected for first year)\n",
      "\n",
      "Cleaned data saved to: data/processed/uba.csv\n",
      "\n",
      "Data structure summary:\n",
      "- Columns: ['year', 'yearName', 'uba', 'ubaGrowthPercentage']\n",
      "- Time series length: 31 years\n",
      "- Data types: {'year': dtype('int64'), 'yearName': dtype('int64'), 'uba': dtype('float64'), 'ubaGrowthPercentage': dtype('float64')}\n"
     ]
    }
   ],
   "source": [
    "# URBAN EXTENT AND CHANGE - uba.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_ubaa\" / \"chart_ubaa\" (absolute urban extent and change)\n",
    "# 2.) \"plot_ubap\" / \"chart_ubap\" (urban extent and change growth percentage)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_uba = pd.read_csv('data/raw/2025-11-mauritania-nouakchott_02-process-output_tabular_nouakchott_wsf_stats.csv') # updatefile path\n",
    "\n",
    "# basic info about raw data\n",
    "print(\"Raw urban built area data info:\")\n",
    "print(f\"Shape: {raw_df_uba.shape}\")\n",
    "print(f\"Columns: {list(raw_df_uba.columns)}\")\n",
    "print(f\"Year range: {raw_df_uba['year'].min()} - {raw_df_uba['year'].max()}\")\n",
    "print(f\"UBA range: {raw_df_uba['cumulative sq km'].min():.2f} - {raw_df_uba['cumulative sq km'].max():.2f} sq km\")\n",
    "print(f\"Total data points: {len(raw_df_uba)}\")\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_uba.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean data using clean_uba function in clean.py\n",
    "try:\n",
    "    cleaned_df_uba = clean_uba('data/raw/2025-11-mauritania-nouakchott_02-process-output_tabular_nouakchott_wsf_stats.csv') # updatefile path\n",
    "    print(\"Urban built area data cleaned successfully!\")\n",
    "    \n",
    "    # cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_uba.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_uba.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_uba.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_uba.isnull().sum().sum()}\")\n",
    "    print(f\"- Year range: {cleaned_df_uba['yearName'].min()} - {cleaned_df_uba['yearName'].max()}\")\n",
    "    print(f\"- UBA range: {cleaned_df_uba['uba'].min():.2f} - {cleaned_df_uba['uba'].max():.2f} sq km\")\n",
    "    print(f\"- Growth rate range: {cleaned_df_uba['ubaGrowthPercentage'].min():.3f}% - {cleaned_df_uba['ubaGrowthPercentage'].max():.3f}%\")\n",
    "    print(f\"- Total urban expansion: {cleaned_df_uba['uba'].max() - cleaned_df_uba['uba'].min():.2f} sq km over {cleaned_df_uba['yearName'].max() - cleaned_df_uba['yearName'].min()} years\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nUrban growth analysis:\")\n",
    "    # calculate average annual growth rate\n",
    "    avg_growth = cleaned_df_uba['ubaGrowthPercentage'].mean()\n",
    "    print(f\"- Average annual UBA growth rate: {avg_growth:.3f}%\")\n",
    "    \n",
    "    # check for any potential data quality issues\n",
    "    if cleaned_df_uba['ubaGrowthPercentage'].isna().sum() > 0:\n",
    "        print(f\"Note: {cleaned_df_uba['ubaGrowthPercentage'].isna().sum()} missing growth rate values (expected for first year)\")\n",
    "    \n",
    "    # check for negative growth (Note: urban area should generally increase)\n",
    "    negative_growth = cleaned_df_uba[cleaned_df_uba['ubaGrowthPercentage'] < 0]\n",
    "    if len(negative_growth) > 0:\n",
    "        print(f\"Warning: {len(negative_growth)} years with negative UBA growth detected\")\n",
    "        print(f\"Years with decline: {negative_growth['yearName'].tolist()}\")\n",
    "      \n",
    "except Exception as e:\n",
    "    print(f\"Error cleaning urban built area data: {e}\")\n",
    "    print(\"Check that the UBA CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: year, cumulative sq km\")\n",
    "\n",
    "# save cleaned data as csv file - uba.csv, and export\n",
    "# (this is handled automatically by clean_uba function, but confirming)\n",
    "if 'cleaned_df_uba' in locals():\n",
    "    print(f\"\\nCleaned data saved to: data/processed/uba.csv\")\n",
    "    \n",
    "    # preview data structure\n",
    "    print(f\"\\nData structure summary:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_uba.columns)}\")\n",
    "    print(f\"- Time series length: {len(cleaned_df_uba)} years\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_uba.dtypes)}\")\n",
    "else:\n",
    "    print(\"No cleaned data available to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a5627",
   "metadata": {},
   "source": [
    "### uba_area.csv preparation (i.e., % area with different years of built-up area urban expansion - \"Before 1985\",\"1986-1995\",\"1996-2005\", and \"2006-2015\")\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_uba_area\" / \"chart_uba_area\" (i.e., Percentage of area with different years of urban built-up area expansion, \"Before 1985\", \"1986-1995\", \"1996-2005\", \"2006-2015\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f8943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URBAN EXTENT AND CHANGE - uba_area.csv preparation from raw tif data for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_uba_area\" / \"chart_uba_area\" (i.e., Percentage of rea with different years of urban built-up area expansion, \"Before 1985\", \"1986-1995\", \"1996-2005\", and \"2006-2015\")\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tif data\n",
    "input_tif_path = 'data/raw/cartagena_main_wsf_evolution_utm.tif'  # updatefile path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"URBAN BUILT-UP AREA EXPANSION DATA PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# process tif file using clean_uba_area function in clean.py\n",
    "try:\n",
    "    cleaned_df_uba = clean_uba_area(input_tif_path)\n",
    "    print(\"Urban expansion data processed successfully!\")\n",
    "    \n",
    "    # cleaned data structure\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_uba.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_uba.columns)}\")\n",
    "    print(f\"\\nProcessed Urban Expansion data:\")\n",
    "    print(cleaned_df_uba)\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData Validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_uba.isnull().sum().sum()}\")\n",
    "    print(f\"- Urban expansion periods: {len(cleaned_df_uba)}\")\n",
    "    print(f\"- Total pixels: {cleaned_df_uba['count'].sum():,.0f}\")\n",
    "    print(f\"- Percentage sum: {cleaned_df_uba['percentage'].sum():.1f}% (should be ~100%)\")\n",
    "    \n",
    "    # urban expansion temporal analysis\n",
    "    print(f\"\\nUrban Expansion Timeline:\")\n",
    "    \n",
    "    total_pixels = cleaned_df_uba['count'].sum()\n",
    "    \n",
    "    for idx, row in cleaned_df_uba.iterrows():\n",
    "        print(f\"- {row['bin']}: {row['count']:,.0f} pixels ({row['percentage']:.1f}%)\")\n",
    "    \n",
    "    # ID most and least active expansion periods\n",
    "    if len(cleaned_df_uba) > 0:\n",
    "        # filter out zero-count periods for meaningful analysis\n",
    "        active_periods = cleaned_df_uba[cleaned_df_uba['count'] > 0]\n",
    "        \n",
    "        if len(active_periods) > 0:\n",
    "            max_expansion = active_periods.loc[active_periods['percentage'].idxmax()]\n",
    "            min_expansion = active_periods.loc[active_periods['percentage'].idxmin()]\n",
    "            \n",
    "            print(f\"\\n- Most active expansion period: {max_expansion['bin']} ({max_expansion['percentage']:.1f}%)\")\n",
    "            print(f\"- Least active expansion period: {min_expansion['bin']} ({min_expansion['percentage']:.1f}%)\")\n",
    "    \n",
    "    # historical vs recent development\n",
    "    before_2000 = cleaned_df_uba[cleaned_df_uba['bin'].isin(['Before 1985', '1986-1995'])]['percentage'].sum()\n",
    "    after_2000 = cleaned_df_uba[cleaned_df_uba['bin'].isin(['1996-2005', '2006-2015'])]['percentage'].sum()\n",
    "    \n",
    "    print(f\"\\nTemporal Distribution:\")\n",
    "    print(f\"- Historical development (before 1996): {before_2000:.1f}%\")\n",
    "    print(f\"- Recent development (1996-2015): {after_2000:.1f}%\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values\n",
    "    missing_values = cleaned_df_uba.isnull().sum().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"Missing values detected: {missing_values}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for negative values (should not exist for counts)\n",
    "    negative_counts = (cleaned_df_uba['count'] < 0).sum()\n",
    "    negative_percentages = (cleaned_df_uba['percentage'] < 0).sum()\n",
    "    \n",
    "    if negative_counts > 0:\n",
    "        print(f\"Negative count values: {negative_counts}\")\n",
    "        quality_issues += 1\n",
    "    if negative_percentages > 0:\n",
    "        print(f\"Negative percentage values: {negative_percentages}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check percentage sum\n",
    "    percentage_sum = cleaned_df_uba['percentage'].sum()\n",
    "    if abs(percentage_sum - 100) > 0.1:\n",
    "        print(f\"Percentage sum deviation: {percentage_sum:.1f}% (should be ~100%)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate periods\n",
    "    duplicates = cleaned_df_uba['bin'].duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Duplicate time periods: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for expected number of periods (should be 4)\n",
    "    if len(cleaned_df_uba) != 4:\n",
    "        print(f\"Unexpected number of periods: {len(cleaned_df_uba)} (expected 4)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"No data quality issues detected\")\n",
    "    \n",
    "    # data structure summary\n",
    "    print(f\"\\nData structure summary:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_uba.columns)}\")\n",
    "    print(f\"- Time periods: {len(cleaned_df_uba)} expansion periods\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_uba.dtypes)}\")\n",
    "    \n",
    "    print(f\"\\nOutput file ready: data/processed/uba_area.csv\")\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing urban expansion data: {e}\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure TIF file exists at the specified path\")\n",
    "    print(\"   2. Check that the TIF file contains valid year data (1900-2030 range)\")\n",
    "    print(\"   3. Verify the TIF file is not corrupted\")\n",
    "    print(\"   4. Ensure rasterio library is installed: pip install rasterio\")\n",
    "    print(\"   5. Check file permissions and disk space\")\n",
    "    print(\"   6. Verify year values in TIF are realistic (e.g., 1985, 2005, etc.)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b914706",
   "metadata": {},
   "source": [
    "### LAND COVER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f620a98",
   "metadata": {},
   "source": [
    "### lc.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_lc\" / \"chart_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "051cbe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw land cover data info:\n",
      "Shape: (11, 2)\n",
      "Columns: ['Land Cover Type', 'Pixel Count']\n",
      "Total data points: 11\n",
      "Land cover types: 11\n",
      "Total pixels: 4,406,066\n",
      "Pixel count range: 0 - 2,913,519\n",
      "Data preview:\n",
      "  Land Cover Type    Pixel Count\n",
      "0      Tree cover    6772.000000\n",
      "1       Shrubland   18797.478431\n",
      "2       Grassland   11182.137255\n",
      "3        Cropland     274.000000\n",
      "4        Built-up  969430.725490\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/lc.csv\n",
      "Land cover types: 8\n",
      "Total pixels analyzed: 4,406,066\n",
      "Percentage coverage verification: 100.0% (should be ~100%)\n",
      "Dominant land cover: Bare / sparse vegetation (66.1%)\n",
      "Land cover data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (8, 4)\n",
      "Cleaned data columns: ['lcType', 'pixelCount', 'pixelTotal', 'percentage']\n",
      "Sample of cleaned data:\n",
      "                     lcType  pixelCount    pixelTotal  percentage\n",
      "0  Bare / sparse vegetation     2913519  4.406066e+06       66.13\n",
      "1                  Built-up      969431  4.406066e+06       22.00\n",
      "2    Permanent water bodies      486087  4.406066e+06       11.03\n",
      "3                 Shrubland       18797  4.406066e+06        0.43\n",
      "4                 Grassland       11182  4.406066e+06        0.25\n",
      "5                Tree cover        6772  4.406066e+06        0.15\n",
      "6                  Cropland         274  4.406066e+06        0.01\n",
      "7        Herbaceous wetland           4  4.406066e+06        0.00\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 0\n",
      "- Land cover types: 8\n",
      "- Total pixels: 4,406,066\n",
      "- Percentage sum: 100.0% (should be ~100%)\n",
      "\n",
      "Land Cover Data Summary:\n",
      "- Land cover types present: 8\n",
      "- Pixel count range: 4 - 2,913,519\n",
      "- Most common land cover: Bare / sparse vegetation (66.1%)\n",
      "- Least common land cover: Herbaceous wetland (0.0%)\n",
      "- Types with ≥10% coverage: 3\n",
      "- Types with ≥5% coverage: 3\n",
      "- Types with ≥1% coverage: 3\n",
      "\n",
      "Data Quality Checks:\n",
      "No data quality issues detected\n",
      "\n",
      "Cleaned data saved to: data/processed/lc.csv\n",
      "\n",
      "Data structure summary for Observable:\n",
      "- Columns: ['lcType', 'pixelCount', 'pixelTotal', 'percentage']\n",
      "- Data points: 8 land cover types\n",
      "- Data types: {'lcType': dtype('O'), 'pixelCount': dtype('int64'), 'pixelTotal': dtype('float64'), 'percentage': dtype('float64')}\n",
      "- Coverage range: 0.0% - 66.1%\n"
     ]
    }
   ],
   "source": [
    "# LAND COVER - lc.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_lc\" / \"chart_lc\" (land cover types)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_lc = pd.read_csv('data/raw/2025-11-mauritania-nouakchott_02-process-output_tabular_nouakchott_lc.csv') # updatefile path\n",
    "\n",
    "# basic info about raw data\n",
    "print(\"Raw land cover data info:\")\n",
    "print(f\"Shape: {raw_df_lc.shape}\")\n",
    "print(f\"Columns: {list(raw_df_lc.columns)}\")\n",
    "print(f\"Total data points: {len(raw_df_lc)}\")\n",
    "\n",
    "# preview land cover types and pixel counts\n",
    "if 'Land Cover Type' in raw_df_lc.columns and 'Pixel Count' in raw_df_lc.columns:\n",
    "    print(f\"Land cover types: {raw_df_lc['Land Cover Type'].nunique()}\")\n",
    "    print(f\"Total pixels: {raw_df_lc['Pixel Count'].sum():,.0f}\")\n",
    "    print(f\"Pixel count range: {raw_df_lc['Pixel Count'].min():,.0f} - {raw_df_lc['Pixel Count'].max():,.0f}\")\n",
    "\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_lc.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean data using clean_lc function in clean.py\n",
    "try:\n",
    "    cleaned_df_lc = clean_lc('data/raw/2025-11-mauritania-nouakchott_02-process-output_tabular_nouakchott_lc.csv') # updatefile path\n",
    "    print(\"Land cover data cleaned successfully!\")\n",
    "    \n",
    "    # cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_lc.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_lc.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_lc.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_lc.isnull().sum().sum()}\")\n",
    "    print(f\"- Land cover types: {len(cleaned_df_lc)}\")\n",
    "    print(f\"- Total pixels: {cleaned_df_lc['pixelTotal'].iloc[0]:,.0f}\")\n",
    "    print(f\"- Percentage sum: {cleaned_df_lc['percentage'].sum():.1f}% (should be ~100%)\")\n",
    "    \n",
    "    # land cover analysis\n",
    "    print(f\"\\nLand Cover Data Summary:\")\n",
    "    \n",
    "    print(f\"- Land cover types present: {len(cleaned_df_lc)}\")\n",
    "    print(f\"- Pixel count range: {cleaned_df_lc['pixelCount'].min():,.0f} - {cleaned_df_lc['pixelCount'].max():,.0f}\")\n",
    "    \n",
    "    # ID extremes\n",
    "    dominant_type = cleaned_df_lc.iloc[0]  # first row after sorting by percentage\n",
    "    least_common_type = cleaned_df_lc.iloc[-1]  # last row after sorting\n",
    "    \n",
    "    print(f\"- Most common land cover: {dominant_type['lcType']} ({dominant_type['percentage']:.1f}%)\")\n",
    "    print(f\"- Least common land cover: {least_common_type['lcType']} ({least_common_type['percentage']:.1f}%)\")\n",
    "    \n",
    "    # coverage distribution\n",
    "    above_10_percent = (cleaned_df_lc['percentage'] >= 10).sum()\n",
    "    above_5_percent = (cleaned_df_lc['percentage'] >= 5).sum()\n",
    "    above_1_percent = (cleaned_df_lc['percentage'] >= 1).sum()\n",
    "    \n",
    "    print(f\"- Types with ≥10% coverage: {above_10_percent}\")\n",
    "    print(f\"- Types with ≥5% coverage: {above_5_percent}\")\n",
    "    print(f\"- Types with ≥1% coverage: {above_1_percent}\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values in key columns\n",
    "    missing_type = cleaned_df_lc['lcType'].isna().sum()\n",
    "    missing_count = cleaned_df_lc['pixelCount'].isna().sum()\n",
    "    missing_percentage = cleaned_df_lc['percentage'].isna().sum()\n",
    "    \n",
    "    if missing_type > 0:\n",
    "        print(f\"Missing land cover type values: {missing_type}\")\n",
    "        quality_issues += 1\n",
    "    if missing_count > 0:\n",
    "        print(f\"Missing pixel count values: {missing_count}\")\n",
    "        quality_issues += 1\n",
    "    if missing_percentage > 0:\n",
    "        print(f\"Missing percentage values: {missing_percentage}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for impossible values\n",
    "    negative_pixels = (cleaned_df_lc['pixelCount'] < 0).sum()\n",
    "    negative_percentage = (cleaned_df_lc['percentage'] < 0).sum()\n",
    "    \n",
    "    if negative_pixels > 0:\n",
    "        print(f\"Negative pixel count values: {negative_pixels}\")\n",
    "        quality_issues += 1\n",
    "    if negative_percentage > 0:\n",
    "        print(f\"Negative percentage values: {negative_percentage}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check percentage sum\n",
    "    percentage_sum = cleaned_df_lc['percentage'].sum()\n",
    "    if abs(percentage_sum - 100) > 0.1:\n",
    "        print(f\"Percentage sum deviation: {percentage_sum:.1f}% (should be ~100%)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate land cover types\n",
    "    duplicates = cleaned_df_lc['lcType'].duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Duplicate land cover types: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"No data quality issues detected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error cleaning land cover data: {e}\")\n",
    "    print(\"Check that the land cover CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: Land Cover Type, Pixel Count\")\n",
    "\n",
    "# save confirmation and next steps\n",
    "if 'cleaned_df_lc' in locals():\n",
    "    print(f\"\\nCleaned data saved to: data/processed/lc.csv\")\n",
    "    \n",
    "    # preview of data structure\n",
    "    print(f\"\\nData structure summary for Observable:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_lc.columns)}\")\n",
    "    print(f\"- Data points: {len(cleaned_df_lc)} land cover types\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_lc.dtypes)}\")\n",
    "    print(f\"- Coverage range: {cleaned_df_lc['percentage'].min():.1f}% - {cleaned_df_lc['percentage'].max():.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"No cleaned land cover data available\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure land cover CSV file exists in data/raw/\")\n",
    "    print(\"   2. Check file has correct columns: Land Cover Type, Pixel Count\")\n",
    "    print(\"   3. Verify pixel count values are numeric and positive\")\n",
    "    print(\"   4. Check that land cover types are properly named\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69282871",
   "metadata": {},
   "source": [
    "# URBAN DEVELOPMENT DYNAMICS MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6bee3c",
   "metadata": {},
   "source": [
    "### pug.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_uddm\" / \"chart_uddm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea7fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URBAN DEVELOPMENT DYNAMICS MATRIX: POPULATION URBAN GROWTH RATIO - pug.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_uddm\" / \"chart_ud\" (population vs urban growth analysis)\n",
    "\n",
    "# prereqs: ensure pg.csv and uba.csv have been generated from clean_pg and clean_uba functions in clean.py\n",
    "print(\"Checking prerequisite files...\")\n",
    "\n",
    "# check if required input files exist\n",
    "import os\n",
    "pg_file = 'data/processed/pg.csv'\n",
    "uba_file = 'data/processed/uba.csv'\n",
    "\n",
    "if os.path.exists(pg_file):\n",
    "    print(f\"Population growth file found: {pg_file}\")\n",
    "else:\n",
    "    print(f\"Population growth file missing: {pg_file}\")\n",
    "    print(\"Run clean_pg function first to generate this file\")\n",
    "\n",
    "if os.path.exists(uba_file):\n",
    "    print(f\"Urban built area file found: {uba_file}\")\n",
    "else:\n",
    "    print(f\"Urban built area file missing: {uba_file}\")\n",
    "    print(\"Run clean_uba function first to generate this file\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean and merge data using clean_pug function in clean.py\n",
    "try:\n",
    "    cleaned_df_pug = clean_pug()  # uses default paths: pg.csv and uba.csv\n",
    "    print(\"Population urban growth data merged and cleaned successfully!\")\n",
    "    \n",
    "    # cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_pug.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_pug.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_pug.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_pug.isnull().sum().sum()}\")\n",
    "    print(f\"- Year range: {cleaned_df_pug['yearName'].min()} - {cleaned_df_pug['yearName'].max()}\")\n",
    "    print(f\"- Population range: {cleaned_df_pug['population'].min():,} - {cleaned_df_pug['population'].max():,}\")\n",
    "    print(f\"- UBA range: {cleaned_df_pug['uba'].min():.2f} - {cleaned_df_pug['uba'].max():.2f} sq km\")\n",
    "    print(f\"- Density range: {cleaned_df_pug['density'].min():.1f} - {cleaned_df_pug['density'].max():.1f} people/sq km\")\n",
    "    \n",
    "    # population vs urban growth analysis\n",
    "    print(f\"\\nPopulation vs Urban Growth Analysis:\")\n",
    "    print(f\"- Population growth rate range: {cleaned_df_pug['populationGrowthPercentage'].min():.3f}% - {cleaned_df_pug['populationGrowthPercentage'].max():.3f}%\")\n",
    "    print(f\"- UBA growth rate range: {cleaned_df_pug['ubaGrowthPercentage'].min():.3f}% - {cleaned_df_pug['ubaGrowthPercentage'].max():.3f}%\")\n",
    "    \n",
    "    # calculate averages (excluding \"NaN\" values)\n",
    "    avg_pop_growth = cleaned_df_pug['populationGrowthPercentage'].mean()\n",
    "    avg_uba_growth = cleaned_df_pug['ubaGrowthPercentage'].mean()\n",
    "    print(f\"- Average annual population growth: {avg_pop_growth:.3f}%\")\n",
    "    print(f\"- Average annual UBA growth: {avg_uba_growth:.3f}%\")\n",
    "    \n",
    "    # growth ratio analysis\n",
    "    valid_ratios = cleaned_df_pug['populationUrbanGrowthRatio'].dropna()\n",
    "    if len(valid_ratios) > 0:\n",
    "        print(f\"- Population/Urban growth ratio range: {valid_ratios.min():.3f} - {valid_ratios.max():.3f}\")\n",
    "        print(f\"- Average growth ratio: {valid_ratios.mean():.3f}\")\n",
    "        \n",
    "        # interpret growth patterns\n",
    "        if valid_ratios.mean() > 1:\n",
    "            print(\"Population growing faster than urban area (potential densification)\")\n",
    "        elif valid_ratios.mean() < 1:\n",
    "            print(\"Urban area growing faster than population (potential sprawl)\")\n",
    "        else:\n",
    "            print(\"Balanced population and urban growth\")\n",
    "    \n",
    "    # density analysis\n",
    "    print(f\"\\nUrban Density Analysis:\")\n",
    "    density_change = cleaned_df_pug['density'].iloc[-1] - cleaned_df_pug['density'].iloc[0]\n",
    "    density_change_pct = (density_change / cleaned_df_pug['density'].iloc[0]) * 100\n",
    "    print(f\"- Starting density ({cleaned_df_pug['yearName'].iloc[0]}): {cleaned_df_pug['density'].iloc[0]:,.1f} people/sq km\")\n",
    "    print(f\"- Ending density ({cleaned_df_pug['yearName'].iloc[-1]}): {cleaned_df_pug['density'].iloc[-1]:,.1f} people/sq km\")\n",
    "    print(f\"- Total density change: {density_change:+.1f} people/sq km ({density_change_pct:+.1f}%)\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    # check for missing ratios\n",
    "    missing_ratios = cleaned_df_pug['populationUrbanGrowthRatio'].isna().sum()\n",
    "    if missing_ratios > 0:\n",
    "        print(f\"Note: {missing_ratios} missing growth ratio values (possibly due to zero UBA growth)\")\n",
    "        zero_uba_growth = cleaned_df_pug[cleaned_df_pug['ubaGrowthPercentage'] == 0]\n",
    "        if len(zero_uba_growth) > 0:\n",
    "            print(f\"Years with zero UBA growth: {zero_uba_growth['yearName'].tolist()}\")\n",
    "    \n",
    "    # check for negative population growth\n",
    "    negative_pop_growth = cleaned_df_pug[cleaned_df_pug['populationGrowthPercentage'] < 0]\n",
    "    if len(negative_pop_growth) > 0:\n",
    "        print(f\"Note: {len(negative_pop_growth)} years with population decline\")\n",
    "        print(f\"Decline years: {negative_pop_growth['yearName'].tolist()}\")\n",
    "    \n",
    "    # check for negative urban growth\n",
    "    negative_uba_growth = cleaned_df_pug[cleaned_df_pug['ubaGrowthPercentage'] < 0]\n",
    "    if len(negative_uba_growth) > 0:\n",
    "        print(f\"Warning: {len(negative_uba_growth)} years with urban area decline\")\n",
    "        print(f\"UBA decline years: {negative_uba_growth['yearName'].tolist()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating population urban growth data: {e}\")\n",
    "    print(\"Check that both pg.csv and uba.csv files exist and have the correct format\")\n",
    "    print(\"Expected pg.csv columns: yearName, population, populationGrowthPercentage\")\n",
    "    print(\"Expected uba.csv columns: yearName, uba, ubaGrowthPercentage\")\n",
    "\n",
    "# save confirmation and next steps\n",
    "if 'cleaned_df_pug' in locals():\n",
    "    print(f\"\\nCleaned data saved to: data/processed/pug.csv\")\n",
    "    \n",
    "    # preview of data structure\n",
    "    print(f\"\\nData structure summary:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_pug.columns)}\")\n",
    "    print(f\"- Time series length: {len(cleaned_df_pug)} years\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_pug.dtypes)}\")\n",
    "    print(f\"- Overlapping years between datasets: {len(cleaned_df_pug)} out of potential maximum\")\n",
    "    \n",
    "    # summary statistics\n",
    "    print(f\"\\nSummary statistics:\")\n",
    "    total_pop_growth = ((cleaned_df_pug['population'].iloc[-1] / cleaned_df_pug['population'].iloc[0]) - 1) * 100\n",
    "    total_uba_growth = ((cleaned_df_pug['uba'].iloc[-1] / cleaned_df_pug['uba'].iloc[0]) - 1) * 100\n",
    "    print(f\"- Total population growth over period: {total_pop_growth:.1f}%\")\n",
    "    print(f\"- Total urban area growth over period: {total_uba_growth:.1f}%\")\n",
    "    print(f\"- NET population density change: {density_change_pct:+.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"No cleaned data available to save\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure pg.csv exists (run clean_pg function)\")\n",
    "    print(\"   2. Ensure uba.csv exists (run clean_uba function)\")\n",
    "    print(\"   3. Check that both files have overlapping years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8fe88a",
   "metadata": {},
   "source": [
    "# CLIMATE CONDITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b5f54",
   "metadata": {},
   "source": [
    "### pv.csv preparation (i.e., monthly max pv potential)\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_pv\" / \"chart_pv\" (i.e., Seasonal availability of solar energy, January - December)\n",
    "#### 2.) \"plot_pv_alt\" / \"chart_pv_alt\" (i.e., Seasonal availability of solar energy, January - December with colored conditions for \"Excellent (4.5+)\", \"Favorable (3.5-4.5)\", and \"Less than Favorable (<3.5)\" conditions)\n",
    "#### 3.) \"plot_pv_d\" / \"chart_pv_d\" (i.e., Photovoltaic Potenital Condition Level Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e3297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHOTOVOLTAIC POTENTIAL - pv.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_pv\" / \"chart_pv\" (i.e., Seasonal availability of solar energy, January - December)\n",
    "# 2.) \"plot_pv_alt\" / \"chart_pv_alt\" (i.e., Seasonal availability of solar energy, January - December with colored conditions for \"Excellent (4.5+)\", \"Favorable (3.5-4.5)\", and \"Less than Favorable (<3.5)\" conditions)\n",
    "# 3.) \"plot_pv_d\" / \"chart_pv_d\" (i.e., Photovoltaic Potenital Condition Level Distribution)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_pv = pd.read_csv('data/raw/monthly-pv.csv') # updatefile path\n",
    "\n",
    "# basic info about raw data\n",
    "print(\"Raw photovoltaic potential data info:\")\n",
    "print(f\"Shape: {raw_df_pv.shape}\")\n",
    "print(f\"Columns: {list(raw_df_pv.columns)}\")\n",
    "print(f\"Month range: {raw_df_pv['month'].min()} - {raw_df_pv['month'].max()}\")\n",
    "print(f\"PV max range: {raw_df_pv['max'].min():.2f} - {raw_df_pv['max'].max():.2f}\")\n",
    "print(f\"PV mean range: {raw_df_pv['mean'].min():.2f} - {raw_df_pv['mean'].max():.2f}\")\n",
    "print(f\"Total data points: {len(raw_df_pv)}\")\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_pv.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean data using clean_pv function in clean.py\n",
    "try:\n",
    "    cleaned_df_pv = clean_pv('data/raw/monthly-pv.csv') # updatefile path\n",
    "    print(\"Photovoltaic potential data cleaned successfully!\")\n",
    "    \n",
    "    # cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_pv.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_pv.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_pv.head(12))  # show all 12 months\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_pv.isnull().sum().sum()}\")\n",
    "    print(f\"- Month coverage: {len(cleaned_df_pv)} months (should be 12)\")\n",
    "    print(f\"- Month range: {cleaned_df_pv['month'].min()} - {cleaned_df_pv['month'].max()}\")\n",
    "    print(f\"- PV potential range: {cleaned_df_pv['maxPv'].min():.2f} - {cleaned_df_pv['maxPv'].max():.2f}\")\n",
    "    \n",
    "    # solar energy analysis\n",
    "    print(f\"\\nSolar Energy Analysis:\")\n",
    "    \n",
    "    # ID peak and low months\n",
    "    peak_month = cleaned_df_pv.loc[cleaned_df_pv['maxPv'].idxmax()]\n",
    "    low_month = cleaned_df_pv.loc[cleaned_df_pv['maxPv'].idxmin()]\n",
    "    \n",
    "    print(f\"- Peak solar month: {peak_month['monthName']} ({peak_month['maxPv']:.2f})\")\n",
    "    print(f\"- Lowest solar month: {low_month['monthName']} ({low_month['maxPv']:.2f})\")\n",
    "    \n",
    "    # seasonal analysis\n",
    "    spring_months = cleaned_df_pv[cleaned_df_pv['month'].isin([3, 4, 5])]  # Mar, Apr, May\n",
    "    summer_months = cleaned_df_pv[cleaned_df_pv['month'].isin([6, 7, 8])]  # Jun, Jul, Aug\n",
    "    fall_months = cleaned_df_pv[cleaned_df_pv['month'].isin([9, 10, 11])]  # Sep, Oct, Nov\n",
    "    winter_months = cleaned_df_pv[cleaned_df_pv['month'].isin([12, 1, 2])]  # Dec, Jan, Feb\n",
    "    \n",
    "    spring_avg = spring_months['maxPv'].mean()\n",
    "    summer_avg = summer_months['maxPv'].mean()\n",
    "    fall_avg = fall_months['maxPv'].mean()\n",
    "    winter_avg = winter_months['maxPv'].mean()\n",
    "    \n",
    "    print(f\"- Spring average (Mar-May): {spring_avg:.2f}\")\n",
    "    print(f\"- Summer average (Jun-Aug): {summer_avg:.2f}\")\n",
    "    print(f\"- Fall average (Sep-Nov): {fall_avg:.2f}\")\n",
    "    print(f\"- Winter average (Dec-Feb): {winter_avg:.2f}\")\n",
    "    \n",
    "    # calculate seasonal variations\n",
    "    annual_avg = cleaned_df_pv['maxPv'].mean()\n",
    "    peak_variation = ((cleaned_df_pv['maxPv'].max() - annual_avg) / annual_avg) * 100 # (i.e., \"the best month 6% better than the average\"\n",
    "    low_variation = ((annual_avg - cleaned_df_pv['maxPv'].min()) / annual_avg) * 100 # (i.e., \"the worst month 10% worse than the average\")\n",
    "    \n",
    "    print(f\"- Annual average: {annual_avg:.2f}\")\n",
    "    print(f\"- Peak month deviation: +{peak_variation:.1f}% above average\")\n",
    "    print(f\"- Low month deviation: -{low_variation:.1f}% below average\")\n",
    "    \n",
    "    # energy planning insights\n",
    "    summer_winter_ratio = summer_avg / winter_avg\n",
    "    print(f\"- Summer/Winter ratio: {summer_winter_ratio:.2f}x\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    # check for complete month coverage\n",
    "    expected_months = set(range(1, 13))\n",
    "    actual_months = set(cleaned_df_pv['month'].unique())\n",
    "    missing_months = expected_months - actual_months\n",
    "    \n",
    "    if missing_months:\n",
    "        print(f\"Warning: Missing months: {sorted(missing_months)}\")\n",
    "    else:\n",
    "        print(\"Complete 12-month coverage\")\n",
    "    \n",
    "    # check for reasonable PV values\n",
    "    if cleaned_df_pv['maxPv'].min() < 0:\n",
    "        print(f\"Warning: Negative PV values detected (minimum: {cleaned_df_pv['maxPv'].min():.2f})\")\n",
    "    \n",
    "    # ID unusual patterns\n",
    "    monthly_diff = cleaned_df_pv['maxPv'].diff().abs()\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error cleaning photovoltaic potential data: {e}\")\n",
    "    print(\"Check that the monthly-pv.csv file exists and has the correct format\")\n",
    "    print(\"Expected columns: month, max, min, mean\")\n",
    "\n",
    "# save cleaned data as csv file - pv.csv, and export\n",
    "# (this is handled automatically by clean_pv function, but confirming)\n",
    "if 'cleaned_df_pv' in locals():\n",
    "    print(f\"\\nCleaned data saved to: data/processed/pv.csv\")\n",
    "    \n",
    "    # preview of data structure\n",
    "    print(f\"\\nData structure summary:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_pv.columns)}\")\n",
    "    print(f\"- Time series type: Monthly (12 data points)\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_pv.dtypes)}\")\n",
    "    print(f\"- Seasonal range: {summer_avg/winter_avg:.2f}x variation from winter to summer\")\n",
    "    \n",
    "    # insights\n",
    "    print(f\"\\nSolar Energy Data Summary:\")\n",
    "    print(f\"- Highest solar months: {', '.join(cleaned_df_pv.nlargest(3, 'maxPv')['monthName'].tolist())}\")\n",
    "    print(f\"- Lowest solar months: {', '.join(cleaned_df_pv.nsmallest(3, 'maxPv')['monthName'].tolist())}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No cleaned data available to save\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure monthly-pv.csv exists in data/raw/\")\n",
    "    print(\"   2. Check file has correct columns: month, max, min, mean\")\n",
    "    print(\"   3. Verify data covers all 12 months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dec587",
   "metadata": {},
   "source": [
    "### pv_area.csv preparation (i.e., % area with different pv conditions - \"Excellent (4+5)\",\"Favorable (3.5-4.5)\",\"Less than Favorable (<3.5)\")\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_pv_area\" / \"chart_pv_area\" (i.e., Percentage of Area with different photovoltaic potential conditions, \"Excellent (4.5+)\", \"Favorable (3.5-4.5)\", and \"Less than Favorable (<3.5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142af5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHOTOVOLTAIC POTENTIAL - pv_area.csv preparation from raw tif data for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_pv_area\" / \"chart_pv_area\" (i.e., Percentage of Area with different photovoltaic potential conditions, \"Excellent (4.5+)\", \"Favorable (3.5-4.5)\", and \"Less than Favorable (<3.5)\")\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tif data\n",
    "input_tif_path = 'data/raw/2025-04-colombia-cartagena_02-process-output_spatial_cartagena_solar.tif' # updatefile path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"photovoltaic potential by area data processing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# process tif file using clean_pv_area function in clean.py\n",
    "try:\n",
    "    cleaned_df_pv = clean_pv_area(input_tif_path)\n",
    "    print(\"photovoltaic potential by area data processed successfully!\")\n",
    "    \n",
    "    # cleaned data structure\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_pv.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_pv.columns)}\")\n",
    "    print(f\"\\nProcessed PV data:\")\n",
    "    print(cleaned_df_pv)\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData Validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_pv.isnull().sum().sum()}\")\n",
    "    print(f\"- PV potential categories: {len(cleaned_df_pv)}\")\n",
    "    print(f\"- Total pixels: {cleaned_df_pv['count'].sum():,.0f}\")\n",
    "    print(f\"- Percentage sum: {cleaned_df_pv['percentage'].sum():.1f}% (should be ~100%)\")\n",
    "    \n",
    "    # pv potential distribution analysis\n",
    "    print(f\"\\nPV Potential Distribution:\")\n",
    "    \n",
    "    # basic statistics\n",
    "    total_pixels = cleaned_df_pv['count'].sum()\n",
    "    \n",
    "    for idx, row in cleaned_df_pv.iterrows():\n",
    "        print(f\"- {row['condition']} ({row['bin']}): {row['count']:,.0f} pixels ({row['percentage']:.1f}%)\")\n",
    "    \n",
    "    # ID most and least favorable areas\n",
    "    if len(cleaned_df_pv) > 0:\n",
    "        max_coverage = cleaned_df_pv.loc[cleaned_df_pv['percentage'].idxmax()]\n",
    "        min_coverage = cleaned_df_pv.loc[cleaned_df_pv['percentage'].idxmin()]\n",
    "        \n",
    "        print(f\"\\n- Dominant condition: {max_coverage['condition']} ({max_coverage['percentage']:.1f}%)\")\n",
    "        print(f\"- Least common condition: {min_coverage['condition']} ({min_coverage['percentage']:.1f}%)\")\n",
    "    \n",
    "    # coverage thresholds\n",
    "    excellent_coverage = cleaned_df_pv[cleaned_df_pv['condition'] == 'Excellent']['percentage'].sum()\n",
    "    favorable_coverage = cleaned_df_pv[cleaned_df_pv['condition'] == 'Favorable']['percentage'].sum()\n",
    "    unfavorable_coverage = cleaned_df_pv[cleaned_df_pv['condition'] == 'Less than Favorable']['percentage'].sum()\n",
    "    \n",
    "    print(f\"\\nPV Potential Summary:\")\n",
    "    print(f\"- Excellent potential areas: {excellent_coverage:.1f}%\")\n",
    "    print(f\"- Favorable potential areas: {favorable_coverage:.1f}%\")\n",
    "    print(f\"- Less favorable areas: {unfavorable_coverage:.1f}%\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values\n",
    "    missing_values = cleaned_df_pv.isnull().sum().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"Missing values detected: {missing_values}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for negative values (note: should NOT exist for pv potential)\n",
    "    negative_counts = (cleaned_df_pv['count'] < 0).sum()\n",
    "    negative_percentages = (cleaned_df_pv['percentage'] < 0).sum()\n",
    "    \n",
    "    if negative_counts > 0:\n",
    "        print(f\"Negative count values: {negative_counts}\")\n",
    "        quality_issues += 1\n",
    "    if negative_percentages > 0:\n",
    "        print(f\"Negative percentage values: {negative_percentages}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check percentage sum\n",
    "    percentage_sum = cleaned_df_pv['percentage'].sum()\n",
    "    if abs(percentage_sum - 100) > 0.1:\n",
    "        print(f\"Percentage sum deviation: {percentage_sum:.1f}% (should be ~100%)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate conditions\n",
    "    duplicates = cleaned_df_pv['condition'].duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Duplicate conditions: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"No data quality issues detected\")\n",
    "    \n",
    "    # data structure summary\n",
    "    print(f\"\\nData structure summary:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_pv.columns)}\")\n",
    "    print(f\"- Categories: {len(cleaned_df_pv)} PV potential levels\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_pv.dtypes)}\")\n",
    "    \n",
    "    print(f\"\\n Output file ready: data/processed/pv_area.csv\")\n",
    "  \n",
    "except Exception as e:\n",
    "    print(f\"Error processing pv potential data: {e}\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure TIF file exists at the specified path\")\n",
    "    print(\"   2. Check that the TIF file contains valid photovoltaic potential data\")\n",
    "    print(\"   3. Verify the TIF file is not corrupted\")\n",
    "    print(\"   4. Ensure rasterio library is installed: pip install rasterio\")\n",
    "    print(\"   5. Check file permissions and disk space\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33049aa1",
   "metadata": {},
   "source": [
    "### AIR QUALITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83865ad",
   "metadata": {},
   "source": [
    "### aq_area.csv preparation (i.e., % area with different air quality, 2019 concentration levels of PM2.5 (µg/mˆ3) - [0-5), [5-10), [10-15), [15-20), [20-30), [30-40), [40-50, [50-100), and [100+])\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_aq_area\" / \"chart_aq_area\" (i.e., Percentage of area with different air quality, 2019 concentration levels of PM2.5 (µg/mˆ3) - [0-5), [5-10), [10-15), [15-20), [20-30), [30-40), [40-50), [50-100), and [100+])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bdddd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AIR QUALITY (PM2.5) DATA PROCESSING\n",
      "============================================================\n",
      "Analyzing TIF data structure...\n",
      "PM2.5 concentration range: 55.30 - 61.20\n",
      "NoData value: -3.4028234663852886e+38\n",
      "Total valid pixels: 290\n",
      "Unique concentration values: 58\n",
      "Mean concentration: 58.02 μg/m³\n",
      "Median concentration: 57.80 μg/m³\n",
      "Standard deviation: 1.25 μg/m³\n",
      "\n",
      "----------------------------------------\n",
      "PM2.5 data range: 55.30 - 61.20 μg/m³\n",
      "Unique values in data: 58\n",
      "Cleaned air quality data saved to: data/processed/aq_area.csv\n",
      "PM2.5 concentration bins: 9\n",
      "Total pixels analyzed: 290\n",
      "Percentage coverage verification: 100.0% (should be ~100%)\n",
      "Most common PM2.5 range: 50-100 μg/m³ (100.0%)\n",
      "Air quality data processed successfully!\n",
      "\n",
      "Cleaned data shape: (9, 3)\n",
      "Cleaned data columns: ['bin', 'count', 'percentage']\n",
      "\n",
      "Processed Air Quality data:\n",
      "      bin  count  percentage\n",
      "0     0-5      0         0.0\n",
      "1    5-10      0         0.0\n",
      "2   10-15      0         0.0\n",
      "3   15-20      0         0.0\n",
      "4   20-30      0         0.0\n",
      "5   30-40      0         0.0\n",
      "6   40-50      0         0.0\n",
      "7  50-100    290       100.0\n",
      "8    100+      0         0.0\n",
      "\n",
      "Data Validation:\n",
      "- Missing values: 0\n",
      "- Concentration bins: 9\n",
      "- Total pixels: 290\n",
      "- Percentage sum: 100.0% (should be ~100%)\n",
      "\n",
      "PM2.5 Concentration Distribution:\n",
      "- 50-100 μg/m³: 290 pixels (100.0%)\n",
      "\n",
      "- Most common concentration range: 50-100 μg/m³ (100.0%)\n",
      "- Least common concentration range: 50-100 μg/m³ (100.0%)\n",
      "\n",
      "Air Quality Summary:\n",
      "- Lower concentrations (0-10 μg/m³): 0.0%\n",
      "- Moderate concentrations (10-20 μg/m³): 0.0%\n",
      "- Higher concentrations (20+ μg/m³): 0.0%\n",
      "\n",
      "Data Quality Checks:\n",
      "No data quality issues detected\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# AIR QUALITY - aq_area.csv preparation from raw tif data for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_aq_area\" / \"chart_aq_area\" (i.e., Percentage of Area with different air quality, 2019 concentration levels of PM2.5 (µg/mˆ3) - [0-5), [5-10), [10-15), [15-20), [20-30), [30-40), [40-50), [50-100), and [100+])\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tif data\n",
    "input_tif_path = 'data/raw/2025-11-mauritania-nouakchott_02-process-output_spatial_nouakchott_air.tif' # updatefile path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AIR QUALITY (PM2.5) DATA PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Analyzing TIF data structure...\")\n",
    "\n",
    "try:\n",
    "    import rasterio\n",
    "    with rasterio.open(input_tif_path) as src:\n",
    "        data = src.read(1)\n",
    "        # remove \"NaN\" and \"NoData\" values for analysis\n",
    "        if src.nodata is not None:\n",
    "            valid_data = data[data != src.nodata]\n",
    "        else:\n",
    "            valid_data = data[~np.isnan(data)]\n",
    "        \n",
    "        # remove negative values (i.e., shouldn't exist for PM2.5)\n",
    "        valid_data = valid_data[valid_data >= 0]\n",
    "        \n",
    "        print(f\"PM2.5 concentration range: {valid_data.min():.2f} - {valid_data.max():.2f}\")\n",
    "        print(f\"NoData value: {src.nodata}\")\n",
    "        print(f\"Total valid pixels: {len(valid_data):,}\")\n",
    "        print(f\"Unique concentration values: {len(np.unique(valid_data)):,}\")\n",
    "        \n",
    "        # basic statistics\n",
    "        print(f\"Mean concentration: {valid_data.mean():.2f} μg/m³\")\n",
    "        print(f\"Median concentration: {np.median(valid_data):.2f} μg/m³\")\n",
    "        print(f\"Standard deviation: {valid_data.std():.2f} μg/m³\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not examine TIF structure: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "\n",
    "# process the tif file using clean_aq_area function in clean.py\n",
    "try:\n",
    "    cleaned_df_aq = clean_aq_area(input_tif_path)\n",
    "    print(\"Air quality data processed successfully!\")\n",
    "    \n",
    "    # cleaned data structure\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_aq.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_aq.columns)}\")\n",
    "    print(f\"\\nProcessed Air Quality data:\")\n",
    "    print(cleaned_df_aq)\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData Validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_aq.isnull().sum().sum()}\")\n",
    "    print(f\"- Concentration bins: {len(cleaned_df_aq)}\")\n",
    "    print(f\"- Total pixels: {cleaned_df_aq['count'].sum():,.0f}\")\n",
    "    print(f\"- Percentage sum: {cleaned_df_aq['percentage'].sum():.1f}% (should be ~100%)\")\n",
    "    \n",
    "    # PM2.5 concentration distribution analysis\n",
    "    print(f\"\\nPM2.5 Concentration Distribution:\")\n",
    "    \n",
    "    total_pixels = cleaned_df_aq['count'].sum()\n",
    "    \n",
    "    for idx, row in cleaned_df_aq.iterrows():\n",
    "        if row['count'] > 0:  # only show bins with data\n",
    "            print(f\"- {row['bin']} μg/m³: {row['count']:,.0f} pixels ({row['percentage']:.1f}%)\")\n",
    "    \n",
    "    # air quality level analysis\n",
    "    if len(cleaned_df_aq) > 0:\n",
    "        # filter out zero-count categories\n",
    "        active_bins = cleaned_df_aq[cleaned_df_aq['count'] > 0]\n",
    "        \n",
    "        if len(active_bins) > 0:\n",
    "            max_concentration_bin = active_bins.loc[active_bins['percentage'].idxmax()]\n",
    "            min_concentration_bin = active_bins.loc[active_bins['percentage'].idxmin()]\n",
    "            \n",
    "            print(f\"\\n- Most common concentration range: {max_concentration_bin['bin']} μg/m³ ({max_concentration_bin['percentage']:.1f}%)\")\n",
    "            print(f\"- Least common concentration range: {min_concentration_bin['bin']} μg/m³ ({min_concentration_bin['percentage']:.1f}%)\")\n",
    "    \n",
    "    # air quality level groupings\n",
    "    good_air = cleaned_df_aq[cleaned_df_aq['bin'].isin(['[0-5)', '[5-10)'])]['percentage'].sum()\n",
    "    moderate_air = cleaned_df_aq[cleaned_df_aq['bin'].isin(['[10-15)', '[15-20)'])]['percentage'].sum()\n",
    "    unhealthy_air = cleaned_df_aq[cleaned_df_aq['bin'].isin(['[20-30)', '[30-40)', '[40-50)', '[50-100)', '100+'])]['percentage'].sum()\n",
    "    \n",
    "    print(f\"\\nAir Quality Summary:\")\n",
    "    print(f\"- Lower concentrations (0-10 μg/m³): {good_air:.1f}%\")\n",
    "    print(f\"- Moderate concentrations (10-20 μg/m³): {moderate_air:.1f}%\")\n",
    "    print(f\"- Higher concentrations (20+ μg/m³): {unhealthy_air:.1f}%\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values\n",
    "    missing_values = cleaned_df_aq.isnull().sum().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"Missing values detected: {missing_values}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for negative values (i.e., should not exist)\n",
    "    negative_counts = (cleaned_df_aq['count'] < 0).sum()\n",
    "    negative_percentages = (cleaned_df_aq['percentage'] < 0).sum()\n",
    "    \n",
    "    if negative_counts > 0:\n",
    "        print(f\"Negative count values: {negative_counts}\")\n",
    "        quality_issues += 1\n",
    "    if negative_percentages > 0:\n",
    "        print(f\"Negative percentage values: {negative_percentages}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check percentage sum\n",
    "    percentage_sum = cleaned_df_aq['percentage'].sum()\n",
    "    if abs(percentage_sum - 100) > 0.1:\n",
    "        print(f\"Percentage sum deviation: {percentage_sum:.1f}% (should be ~100%)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate bins\n",
    "    duplicates = cleaned_df_aq['bin'].duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Duplicate concentration bins: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for expected number of bins (should be 9)\n",
    "    if len(cleaned_df_aq) != 9:\n",
    "        print(f\"Unexpected number of bins: {len(cleaned_df_aq)} (expected 9)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"No data quality issues detected\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing air quality data: {e}\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure TIF file exists at the specified path\")\n",
    "    print(\"   2. Check that the TIF file contains PM2.5 concentration values\")\n",
    "    print(\"   3. Verify the TIF file is not corrupted\")\n",
    "    print(\"   4. Ensure rasterio library is installed: pip install rasterio\")\n",
    "    print(\"   5. Check file permissions and disk space\")\n",
    "    print(\"   6. Verify concentration values are realistic (0-500+ μg/m³)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8cad9b",
   "metadata": {},
   "source": [
    "### SUMMER SURFACE TEMPERATURE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4fcbac",
   "metadata": {},
   "source": [
    "### summer_area.csv preparation (i.e., % area with different summer surface temperatures in degrees Celsius (°C))\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_summer_area\" / \"chart_summer_area\" (i.e., Percentage of area with different summer surface temperatures in degrees Celsius (°C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405575cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMER SURFACE TEMPERATURE - summer_area.csv preparation from raw tif data for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_summer_area\" / \"chart_summer_area\" (i.e., Percentage of Area with different summer surface temperatures in degrees Celsius (°C))\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tif data\n",
    "input_tif_path = 'data/raw/2025-04-colombia-cartagena_02-process-output_spatial_cartagena_summer.tif' # updatefile path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SUMMER SURFACE TEMPERATURE DATA PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Analyzing TIF data structure...\")\n",
    "\n",
    "try:\n",
    "    import rasterio\n",
    "    with rasterio.open(input_tif_path) as src:\n",
    "        data = src.read(1)\n",
    "        # remove \"NaN\" and \"NoData\" values for analysis\n",
    "        if src.nodata is not None:\n",
    "            valid_data = data[data != src.nodata]\n",
    "        else:\n",
    "            valid_data = data[~np.isnan(data)]\n",
    "        \n",
    "        # remove infinite values\n",
    "        valid_data = valid_data[np.isfinite(valid_data)]\n",
    "        \n",
    "        print(f\"Temperature range: {valid_data.min():.1f}°C - {valid_data.max():.1f}°C\")\n",
    "        print(f\"NoData value: {src.nodata}\")\n",
    "        print(f\"Total valid pixels: {len(valid_data):,}\")\n",
    "        print(f\"Unique temperature values: {len(np.unique(valid_data)):,}\")\n",
    "        \n",
    "        # show basic statistics\n",
    "        print(f\"Mean temperature: {valid_data.mean():.1f}°C\")\n",
    "        print(f\"Median temperature: {np.median(valid_data):.1f}°C\")\n",
    "        print(f\"Standard deviation: {valid_data.std():.1f}°C\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not examine TIF structure: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "\n",
    "# process the tif file using clean_summer_area function in clean.py\n",
    "# bin_width=5 creates 5°C temperature bins (default)\n",
    "try:\n",
    "    cleaned_df_summer = clean_summer_area(input_tif_path, bin_width=5)\n",
    "    print(\"Summer temperature data processed successfully!\")\n",
    "    \n",
    "    # display cleaned data structure\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_summer.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_summer.columns)}\")\n",
    "    print(f\"\\nProcessed Summer Temperature data:\")\n",
    "    print(cleaned_df_summer)\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData Validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_summer.isnull().sum().sum()}\")\n",
    "    print(f\"- Temperature bins: {len(cleaned_df_summer)}\")\n",
    "    print(f\"- Total pixels: {cleaned_df_summer['count'].sum():,.0f}\")\n",
    "    print(f\"- Percentage sum: {cleaned_df_summer['percentage'].sum():.1f}% (should be ~100%)\")\n",
    "    \n",
    "    # temperature distribution analysis\n",
    "    print(f\"\\nTemperature Distribution:\")\n",
    "    \n",
    "    total_pixels = cleaned_df_summer['count'].sum()\n",
    "    \n",
    "    for idx, row in cleaned_df_summer.iterrows():\n",
    "        if row['count'] > 0:  # only show bins with data\n",
    "            print(f\"- {row['bin']}°C: {row['count']:,.0f} pixels ({row['percentage']:.1f}%)\")\n",
    "    \n",
    "    # temperature range analysis\n",
    "    if len(cleaned_df_summer) > 0:\n",
    "        # filter out zero-count bins\n",
    "        active_bins = cleaned_df_summer[cleaned_df_summer['count'] > 0]\n",
    "        \n",
    "        if len(active_bins) > 0:\n",
    "            max_temp_bin = active_bins.loc[active_bins['percentage'].idxmax()]\n",
    "            min_temp_bin = active_bins.loc[active_bins['percentage'].idxmin()]\n",
    "            \n",
    "            print(f\"\\n- Most common temperature range: {max_temp_bin['bin']}°C ({max_temp_bin['percentage']:.1f}%)\")\n",
    "            print(f\"- Least common temperature range: {min_temp_bin['bin']}°C ({min_temp_bin['percentage']:.1f}%)\")\n",
    "    \n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values\n",
    "    missing_values = cleaned_df_summer.isnull().sum().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"Missing values detected: {missing_values}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for negative values in \"count\"/\"percentage\"\n",
    "    negative_counts = (cleaned_df_summer['count'] < 0).sum()\n",
    "    negative_percentages = (cleaned_df_summer['percentage'] < 0).sum()\n",
    "    \n",
    "    if negative_counts > 0:\n",
    "        print(f\"Negative count values: {negative_counts}\")\n",
    "        quality_issues += 1\n",
    "    if negative_percentages > 0:\n",
    "        print(f\"Negative percentage values: {negative_percentages}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check percentage sum\n",
    "    percentage_sum = cleaned_df_summer['percentage'].sum()\n",
    "    if abs(percentage_sum - 100) > 0.1:\n",
    "        print(f\"Percentage sum deviation: {percentage_sum:.1f}% (should be ~100%)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate bins\n",
    "    duplicates = cleaned_df_summer['bin'].duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Duplicate temperature bins: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"No data quality issues detected\")\n",
    "      \n",
    "except Exception as e:\n",
    "    print(f\"Error processing summer temperature data: {e}\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure TIF file exists at the specified path\")\n",
    "    print(\"   2. Check that the TIF file contains temperature data in Celsius\")\n",
    "    print(\"   3. Verify the TIF file is not corrupted\")\n",
    "    print(\"   4. Ensure rasterio library is installed: pip install rasterio\")\n",
    "    print(\"   5. Check file permissions and disk space\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d25e72",
   "metadata": {},
   "source": [
    "### Green Space, NDVI (Normalized Difference Vegetated Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1f5aa8",
   "metadata": {},
   "source": [
    "### ndvi_area.csv preparation (i.e., percentage area with different NDVI - - i.e., \"Water\", [-1-0.015); \"Built-up\", [0.015-0.14); \"Barren\", [0.14-0.18); \"Shrub and Grassland\", [0.18-0.27); \"Sparse\", [0.27-0.36); and \"Dense\",   [0.36-1])\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_ndvi_area\" / \"chart_ndvi_area\" (i.e., percentage area with different NDVI - - i.e., \"Water\", [-1-0.015); \"Built-up\", [0.015-0.14); \"Barren\", [0.14-0.18); \"Shrub and Grassland\", [0.18-0.27); \"Sparse\", [0.27-0.36); and \"Dense\",   [0.36-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GREEN SPACE, NDVI (Normalized Difference Vegetated Index) - ndvi_area.csv preparation from raw tif data for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_ndvi_area\" / \"chart_ndvi_area\" (i.e., percentage area with different NDVI - - i.e., \"Water\", [-1-0.015); \"Built-up\", [0.015-0.14); \"Barren\", [0.14-0.18); \"Shrub and Grassland\", [0.18-0.27); \"Sparse\", [0.27-0.36); and \"Dense\",   [0.36-1])\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tif data\n",
    "input_tif_path = 'data/raw/2025-04-colombia-cartagena_02-process-output_spatial_cartagena_ndvi_season.tif' # updatefile path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"NDVI GREEN SPACE DATA PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Analyzing TIF data structure...\")\n",
    "\n",
    "try:\n",
    "    import rasterio\n",
    "    with rasterio.open(input_tif_path) as src:\n",
    "        data = src.read(1)\n",
    "        # remove \"NaN\" and \"NoData\" values for analysis\n",
    "        if src.nodata is not None:\n",
    "            valid_data = data[data != src.nodata]\n",
    "        else:\n",
    "            valid_data = data[~np.isnan(data)]\n",
    "        \n",
    "        # remove infinite values\n",
    "        valid_data = valid_data[np.isfinite(valid_data)]\n",
    "        \n",
    "        print(f\"NDVI range: {valid_data.min():.3f} - {valid_data.max():.3f}\")\n",
    "        print(f\"NoData value: {src.nodata}\")\n",
    "        print(f\"Total valid pixels: {len(valid_data):,}\")\n",
    "        print(f\"Unique NDVI values: {len(np.unique(valid_data)):,}\")\n",
    "        \n",
    "        # show some basic statistics\n",
    "        print(f\"Mean NDVI: {valid_data.mean():.3f}\")\n",
    "        print(f\"Median NDVI: {np.median(valid_data):.3f}\")\n",
    "        print(f\"Standard deviation: {valid_data.std():.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not examine TIF structure: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "\n",
    "# process the tif file using \"clean_ndvi_area\" function in clean.py\n",
    "try:\n",
    "    cleaned_df_ndvi = clean_ndvi_area(input_tif_path)\n",
    "    print(\"NDVI data processed successfully!\")\n",
    "    \n",
    "    # cleaned data structure\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_ndvi.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_ndvi.columns)}\")\n",
    "    print(f\"\\nProcessed NDVI data:\")\n",
    "    print(cleaned_df_ndvi)\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData Validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_ndvi.isnull().sum().sum()}\")\n",
    "    print(f\"- Vegetation categories: {len(cleaned_df_ndvi)}\")\n",
    "    print(f\"- Total pixels: {cleaned_df_ndvi['count'].sum():,.0f}\")\n",
    "    print(f\"- Percentage sum: {cleaned_df_ndvi['percentage'].sum():.1f}% (should be ~100%)\")\n",
    "    \n",
    "    # NDVI vegetation distribution analysis\n",
    "    print(f\"\\nVegetation Cover Distribution:\")\n",
    "    \n",
    "    total_pixels = cleaned_df_ndvi['count'].sum()\n",
    "    \n",
    "    for idx, row in cleaned_df_ndvi.iterrows():\n",
    "        if row['count'] > 0:  # only show categories with data\n",
    "            print(f\"- {row['type']} {row['bin']}: {row['count']:,.0f} pixels ({row['percentage']:.1f}%)\")\n",
    "    \n",
    "    # vegetation type analysis\n",
    "    if len(cleaned_df_ndvi) > 0:\n",
    "        # filter out zero-count categories\n",
    "        active_categories = cleaned_df_ndvi[cleaned_df_ndvi['count'] > 0]\n",
    "        \n",
    "        if len(active_categories) > 0:\n",
    "            max_vegetation = active_categories.loc[active_categories['percentage'].idxmax()]\n",
    "            min_vegetation = active_categories.loc[active_categories['percentage'].idxmin()]\n",
    "            \n",
    "            print(f\"\\n- Most common cover: {max_vegetation['type']} ({max_vegetation['percentage']:.1f}%)\")\n",
    "            print(f\"- Least common cover: {min_vegetation['type']} ({min_vegetation['percentage']:.1f}%)\")\n",
    "    \n",
    "    # green space analysis\n",
    "    dense_vegetation = cleaned_df_ndvi[cleaned_df_ndvi['type'] == 'Dense']['percentage'].sum()\n",
    "    sparse_vegetation = cleaned_df_ndvi[cleaned_df_ndvi['type'] == 'Sparse']['percentage'].sum()\n",
    "    shrub_grassland_vegetation = cleaned_df_ndvi[cleaned_df_ndvi['type'] == 'Shrub and Grassland']['percentage'].sum()\n",
    "    total_green_space = dense_vegetation + sparse_vegetation + shrub_grassland_vegetation\n",
    "\n",
    "    # non green space analysis\n",
    "    barren_space = cleaned_df_ndvi[cleaned_df_ndvi['type'] == 'Barren']['percentage'].sum()\n",
    "    built_up_space = cleaned_df_ndvi[cleaned_df_ndvi['type'] == 'Built-up']['percentage'].sum()\n",
    "    water_space = cleaned_df_ndvi[cleaned_df_ndvi['type'] == 'Water']['percentage'].sum()\n",
    "    total_non_green_space = barren_space + built_up_space + water_space\n",
    "\n",
    "    \n",
    "    print(f\"\\nGreen Space Analysis:\")\n",
    "    print(f\"- Dense vegetation coverage: {dense_vegetation:.1f}%\")\n",
    "    print(f\"- Sparse vegetation coverage: {sparse_vegetation:.1f}%\")\n",
    "    print(f\"- Shrub and Grassland vegetation coverage: {shrub_grassland_vegetation:.1f}%\")\n",
    "    print(f\"- Total green space coverage: {total_green_space:.1f}%\")\n",
    "\n",
    "    print(f\"\\nNon Green Space Analysis:\")\n",
    "    print(f\"- Barren coverage: {barren_space:.1f}%\")\n",
    "    print(f\"- Built-up coverage: {built_up_space:.1f}%\")\n",
    "    print(f\"- Water coverage: {water_space:.1f}%\")\n",
    "    print(f\"- Total non green space coverage: {total_non_green_space:.1f}%\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values\n",
    "    missing_values = cleaned_df_ndvi.isnull().sum().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"Missing values detected: {missing_values}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for negative values in \"count\"/\"percentage\"\n",
    "    negative_counts = (cleaned_df_ndvi['count'] < 0).sum()\n",
    "    negative_percentages = (cleaned_df_ndvi['percentage'] < 0).sum()\n",
    "    \n",
    "    if negative_counts > 0:\n",
    "        print(f\"Negative count values: {negative_counts}\")\n",
    "        quality_issues += 1\n",
    "    if negative_percentages > 0:\n",
    "        print(f\"Negative percentage values: {negative_percentages}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check percentage sum\n",
    "    percentage_sum = cleaned_df_ndvi['percentage'].sum()\n",
    "    if abs(percentage_sum - 100) > 0.1:\n",
    "        print(f\"Percentage sum deviation: {percentage_sum:.1f}% (should be ~100%)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate categories\n",
    "    duplicates_bin = cleaned_df_ndvi['bin'].duplicated().sum()\n",
    "    duplicates_type = cleaned_df_ndvi['type'].duplicated().sum()\n",
    "    \n",
    "    if duplicates_bin > 0:\n",
    "        print(f\"Duplicate NDVI bins: {duplicates_bin}\")\n",
    "        quality_issues += 1\n",
    "    if duplicates_type > 0:\n",
    "        print(f\"Duplicate vegetation types: {duplicates_type}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for expected number of categories (i.e., \"type\"s) (should be 6)\n",
    "    if len(cleaned_df_ndvi) != 6:\n",
    "        print(f\"Unexpected number of categories: {len(cleaned_df_ndvi)} (expected 6)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for expected vegetation types\n",
    "    expected_types = ['Water', 'Built-up', 'Barren', 'Shrub and Grassland', 'Sparse', 'Dense']\n",
    "    actual_types = cleaned_df_ndvi['type'].tolist()\n",
    "    missing_types = set(expected_types) - set(actual_types)\n",
    "    if missing_types:\n",
    "        print(f\"Categories not present in data: {missing_types}\")\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"No data quality issues detected\")\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing NDVI data: {e}\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure TIF file exists at the specified path\")\n",
    "    print(\"   2. Check that the TIF file contains NDVI values (-1 to 1 range)\")\n",
    "    print(\"   3. Verify the TIF file is not corrupted\")\n",
    "    print(\"   4. Ensure rasterio library is installed: pip install rasterio\")\n",
    "    print(\"   5. Check file permissions and disk space\")\n",
    "    print(\"   6. Verify NDVI values are realistic (-1.0 to 1.0 range)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b87a51",
   "metadata": {},
   "source": [
    "### Forests and Deforestation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e4849",
   "metadata": {},
   "source": [
    "### deforestation_area.csv preparation (i.e., percentage area that is forested and deforested (by year)\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_deforestation_area1\" / \"chart_deforestation_area1\" (i.e., cumulative forest loss over time, 2000-2023)\n",
    "#### 2.) \"plot_deforestation_area2\" / \"chart_deforestation_area2\" (i.e., cumulative forest remaining over time, 2000-2023)\n",
    "#### 3.) \"plot_deforestation_area3\" / \"chart_deforestation_area3\" (i.e., annual deforestation as percentage of original forest)\n",
    "#### 4.) \"plot_deforestation_area4\" / \"chart_deforestation_area4\" (i.e., annual deforestation as percentage of total deforestation)\n",
    "#### 5.) \"plot_deforestation_area5\" / \"chart_deforestation_area5\" (i.e., year-by-year forest composition)\n",
    "#### 6.) \"plot_deforestation_area6\" / \"chart_deforestation_area6\" (i.e., progression of deforestation over time)\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f26a2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FOREST COVER & DEFORESTATION DATA PROCESSING\n",
      "============================================================\n",
      "Alignment check:\n",
      "- Same CRS: True\n",
      "- Same shape: True\n",
      "- Same bounds: True\n",
      "- Fully aligned: True\n",
      "TIFs are properly aligned\n",
      "\n",
      "Baseline forest area: 0 pixels\n",
      "\n",
      "Cleaned deforestation data saved to: data/processed/deforestation_area.csv\n",
      "Time period: 2000 - 2000.0\n",
      "Baseline forest: 0 pixels\n",
      "Total deforested: 0.0 pixels (0.00%)\n",
      "Forest remaining: 0.0 pixels (100.00%)\n",
      "Error processing deforestation data: 0\n",
      "\n",
      "Troubleshooting:\n",
      "1. Check file paths are correct\n",
      "2. Ensure rasterio is installed: pip install rasterio\n",
      "3. Verify TIF files are valid and not corrupted\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# FORESTS AND DEFORESTATION - deforestation_area.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_deforestation_area1\" / \"chart_deforestation_area1\" (i.e., cumulative forest loss over time, 2000-2023)\n",
    "# 2.) \"plot_deforestation_area2\" / \"chart_deforestation_area2\" (i.e., cumulative forest remaining over time, 2000-2023)\n",
    "#3.) \"plot_deforestation_area3\" / \"chart_deforestation_area3\" (i.e., annual deforestation as percentage of original forest)\n",
    "# 4.) \"plot_deforestation_area4\" / \"chart_deforestation_area4\" (i.e., annual deforestation as percentage of total deforestation)\n",
    "#5.) \"plot_deforestation_area5\" / \"chart_deforestation_area5\" (i.e., year-by-year forest composition)\n",
    "#6.) \"plot_deforestation_area6\" / \"chart_deforestation_area6\" (i.e., progression of deforestation over time)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tif data\n",
    "forest_tif_path = 'data/raw/2025-11-mauritania-nouakchott_02-process-output_spatial_nouakchott_forest_cover23.tif' # updatefile path\n",
    "deforestation_tif_path = 'data/raw/2025-11-mauritania-nouakchott_02-process-output_spatial_nouakchott_deforestation.tif' # updatefile path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FOREST COVER & DEFORESTATION DATA PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# process tif files using \"clean_deforestation_area\" function in clean.py\n",
    "# \"base_year=2000\" means that value \"1\" = 2001; and  value \"23\" = 2023\n",
    "# note: \"auto_align=True\" will automatically fix any alignment issues\n",
    "try:\n",
    "    cleaned_df_deforest = clean_deforestation_area(\n",
    "        forest_tif_file=forest_tif_path,\n",
    "        deforestation_tif_file=deforestation_tif_path,\n",
    "        base_year=2000,\n",
    "        auto_align=True\n",
    "    )\n",
    "    print(\"\\nDeforestation data processed successfully!\")\n",
    "    \n",
    "    # cleaned data structure\n",
    "    print(f\"\\nYear-over-Year Deforestation Data:\")\n",
    "    print(cleaned_df_deforest.to_string(index=False))\n",
    "    \n",
    "    # data validation\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA VALIDATION:\")\n",
    "    print(f\"- Total years tracked: {len(cleaned_df_deforest)}\")\n",
    "    print(f\"- Missing values: {cleaned_df_deforest.isnull().sum().sum()}\")\n",
    "    \n",
    "    # check that percentages add up correctly (should be 100%)\n",
    "    final_row = cleaned_df_deforest.iloc[-1]\n",
    "    percent_check = final_row['percent_forest_remaining'] + final_row['percent_forest_lost']\n",
    "    print(f\"- Percent remaining + lost: {percent_check:.1f}% (should be 100%)\")\n",
    "    \n",
    "    # deforestation trend analysis\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"DEFORESTATION TRENDS:\")\n",
    "    \n",
    "    years_with_deforest = cleaned_df_deforest[cleaned_df_deforest['deforested_this_year'] > 0]\n",
    "    \n",
    "    if len(years_with_deforest) > 0:\n",
    "        # overall statistics\n",
    "        total_deforested = final_row['cumulative_deforested']\n",
    "        avg_annual = years_with_deforest['deforested_this_year'].mean()\n",
    "        \n",
    "        print(f\"- Years with deforestation: {len(years_with_deforest)}\")\n",
    "        print(f\"- Average annual loss: {avg_annual:.0f} pixels/year\")\n",
    "        print(f\"- Total forest lost: {total_deforested:,} pixels ({final_row['percent_forest_lost']:.2f}%)\")\n",
    "        \n",
    "        # peak year of deforestation\n",
    "        peak = years_with_deforest.loc[years_with_deforest['deforested_this_year'].idxmax()]\n",
    "        print(f\"- Peak deforestation: {peak['year']} ({peak['deforested_this_year']:,} pixels)\")\n",
    "        \n",
    "        # recent deforestation trend (i.e., last 5 years)\n",
    "        recent_5 = years_with_deforest.tail(5)\n",
    "        recent_total = recent_5['deforested_this_year'].sum()\n",
    "        recent_avg = recent_5['deforested_this_year'].mean()\n",
    "        \n",
    "        print(f\"\\nRecent 5 Years (last available data):\")\n",
    "        print(f\"- Total deforested: {recent_total:,} pixels\")\n",
    "        print(f\"- Average annual: {recent_avg:.0f} pixels/year\")\n",
    "        \n",
    "        # compare recent deforestation vs overall average deforestation\n",
    "        # note:\n",
    "        # ratio = recent 5 years (pixel/year) / overall historical average (pixels/year), example: 13(pixel/year) / 18(pixel/year) = 0.7x overall average - SLOWING!\n",
    "\n",
    "        # ratio is > 1, recent deforestaion is FASTER than historic average deforestation (i.e., accelerating);\n",
    "        # ratio is < 1, recent deforestaion is SLOWER than historic average deforestation (i.e., decelerating);\n",
    "        # ratio is = 1, recent deforestaion is at the SAME rate as historic average deforestation (i.e., steady);\n",
    "        if recent_avg > avg_annual:\n",
    "            print(f\"- Trend: Accelerating ({recent_avg/avg_annual:.1f}x overall average)\")\n",
    "        else:\n",
    "            print(f\"- Trend: Slowing ({recent_avg/avg_annual:.1f}x overall average)\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA QUALITY CHECKS:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for negative values\n",
    "    if (cleaned_df_deforest['forest_remaining'] < 0).any():\n",
    "        print(\"Negative forest remaining values detected\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check that cumulative never decreases\n",
    "    cumulative_diff = cleaned_df_deforest['cumulative_deforested'].diff()\n",
    "    if (cumulative_diff[cumulative_diff.notna()] < 0).any():\n",
    "        print(\"Cumulative deforestation decreases (impossible)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check that forest remaining + cumulative = baseline\n",
    "    baseline = cleaned_df_deforest.iloc[0]['forest_remaining']\n",
    "    for idx, row in cleaned_df_deforest.iterrows():\n",
    "        if row['forest_remaining'] + row['cumulative_deforested'] != baseline:\n",
    "            print(f\"Year {row['year']}: Forest accounting error\")\n",
    "            quality_issues += 1\n",
    "            break\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"No data quality issues detected\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing deforestation data: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check file paths are correct\")\n",
    "    print(\"2. Ensure rasterio is installed: pip install rasterio\")\n",
    "    print(\"3. Verify TIF files are valid and not corrupted\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05049de7",
   "metadata": {},
   "source": [
    "# RISK IDENTIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c219496",
   "metadata": {},
   "source": [
    "### FLOOD EVENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e10d1",
   "metadata": {},
   "source": [
    "### fu.csv, pu.csv, cu.csv, and comb.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_fu\" / \"chart_fu\" (i.e., built-up area exposed to river (fluvial) flooding)\n",
    "#### 2.) \"plot_pu\" / \"chart_pu\" (i.e., built-up area exposed to rainwater (pluvial) flooding)\n",
    "#### 3.) \"plot_cu\" / \"chart_cu\" (i.e., built-up area exposed to coastal flooding)\n",
    "#### 4.) \"plot_comb\" / \"chart_comb\" (i.e., built-up area exposed to combined flooding)\n",
    "\n",
    "#### NOTE:\n",
    "#### 5.) data, raw, \"flood-events.csv\" is already ready for Observable Notebook \"plot_fe\" / \"chart_fe\" (i.e.,large flood events in city, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f296df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw flood risk data info:\n",
      "Shape: (31, 5)\n",
      "Columns: ['year', 'coastal_2020', 'comb_2020', 'fluvial_2020', 'pluvial_2020']\n",
      "Year range: 1985 - 2015\n",
      "Total data points: 31\n",
      "Available flood types: ['coastal_2020', 'comb_2020', 'fluvial_2020', 'pluvial_2020']\n",
      "Coastal risk range: 0.00 - 0.11\n",
      "Comb risk range: 9.16 - 39.56\n",
      "Fluvial risk range: 0.00 - 0.00\n",
      "Pluvial risk range: 9.16 - 39.45\n",
      "Data preview:\n",
      "   year  coastal_2020  comb_2020  fluvial_2020  pluvial_2020\n",
      "0  1985      0.000000   9.155106           0.0      9.155106\n",
      "1  1986      0.000000   9.155106           0.0      9.155106\n",
      "2  1987      0.000861  11.003510           0.0     11.002650\n",
      "3  1988      0.001721  12.592896           0.0     12.591175\n",
      "4  1989      0.002582  13.657364           0.0     13.654782\n",
      "\n",
      "==================================================\n",
      "\n",
      "Available flood types: ['coastal', 'fluvial', 'pluvial', 'combined']\n",
      "Created cu.csv: 31 records\n",
      "   Year range: 1985 - 2015\n",
      "   CU range: 0.00 - 0.11\n",
      "Created fu.csv: 31 records\n",
      "   Year range: 1985 - 2015\n",
      "   FU range: 0.00 - 0.00\n",
      "Created pu.csv: 31 records\n",
      "   Year range: 1985 - 2015\n",
      "   PU range: 9.16 - 39.45\n",
      "Created comb.csv: 31 records\n",
      "   Year range: 1985 - 2015\n",
      "   COMB range: 9.16 - 39.56\n",
      "\n",
      "Flood Risk Data Processing Summary:\n",
      "- Input file: data/raw/2025-11-mauritania-nouakchott_02-process-output_tabular_nouakchott_flood_wsf.csv\n",
      "- Output directory: data/processed\n",
      "- Files created: cu.csv, fu.csv, pu.csv, comb.csv\n",
      "- Missing flood types: set()\n",
      "\n",
      "Flood Risk Analysis:\n",
      "- Coastal flood risk:\n",
      "  Average: 0.04, Range: 0.00 - 0.11\n",
      "  Trend (1985-2015): +0.11 (+increase)\n",
      "- Fluvial flood risk:\n",
      "  Average: 0.00, Range: 0.00 - 0.00\n",
      "  Trend (1985-2015): +0.00 (stable)\n",
      "- Pluvial flood risk:\n",
      "  Average: 24.39, Range: 9.16 - 39.45\n",
      "  Trend (1985-2015): +30.29 (+increase)\n",
      "- Combined flood risk:\n",
      "  Average: 24.44, Range: 9.16 - 39.56\n",
      "  Trend (1985-2015): +30.41 (+increase)\n",
      "- Dominant risk type (2015): Combined (39.56)\n",
      "Flood risk data processed successfully!\n",
      "\n",
      "cu.csv validation:\n",
      "- Shape: (31, 3)\n",
      "- Columns: ['year', 'yearName', 'cu']\n",
      "- Year range: 1985 - 2015\n",
      "- Risk range: 0.00 - 0.11\n",
      "- Sample data:\n",
      "   year  yearName   cu\n",
      "0     1      1985  0.0\n",
      "1     2      1986  0.0\n",
      "2     3      1987  0.0\n",
      "3     4      1988  0.0\n",
      "4     5      1989  0.0\n",
      "\n",
      "fu.csv validation:\n",
      "- Shape: (31, 3)\n",
      "- Columns: ['year', 'yearName', 'fu']\n",
      "- Year range: 1985 - 2015\n",
      "- Risk range: 0.00 - 0.00\n",
      "- Sample data:\n",
      "   year  yearName   fu\n",
      "0     1      1985  0.0\n",
      "1     2      1986  0.0\n",
      "2     3      1987  0.0\n",
      "3     4      1988  0.0\n",
      "4     5      1989  0.0\n",
      "\n",
      "pu.csv validation:\n",
      "- Shape: (31, 3)\n",
      "- Columns: ['year', 'yearName', 'pu']\n",
      "- Year range: 1985 - 2015\n",
      "- Risk range: 9.16 - 39.45\n",
      "- Sample data:\n",
      "   year  yearName     pu\n",
      "0     1      1985   9.16\n",
      "1     2      1986   9.16\n",
      "2     3      1987  11.00\n",
      "3     4      1988  12.59\n",
      "4     5      1989  13.65\n",
      "\n",
      "comb.csv validation:\n",
      "- Shape: (31, 3)\n",
      "- Columns: ['year', 'yearName', 'comb']\n",
      "- Year range: 1985 - 2015\n",
      "- Risk range: 9.16 - 39.56\n",
      "- Sample data:\n",
      "   year  yearName   comb\n",
      "0     1      1985   9.16\n",
      "1     2      1986   9.16\n",
      "2     3      1987  11.00\n",
      "3     4      1988  12.59\n",
      "4     5      1989  13.66\n",
      "\n",
      "Flood Risk Data Summary:\n",
      "\n",
      "- CU flood risk:\n",
      "  Average: 0.04\n",
      "  Range: 0.00 - 0.11\n",
      "  Standard deviation: 0.04\n",
      "  Change (1985-2015): +0.11\n",
      "\n",
      "- FU flood risk:\n",
      "  Average: 0.00\n",
      "  Range: 0.00 - 0.00\n",
      "  Standard deviation: 0.00\n",
      "  Change (1985-2015): +0.00\n",
      "\n",
      "- PU flood risk:\n",
      "  Average: 24.39\n",
      "  Range: 9.16 - 39.45\n",
      "  Standard deviation: 9.74\n",
      "  Change (1985-2015): +30.29\n",
      "\n",
      "- COMB flood risk:\n",
      "  Average: 24.44\n",
      "  Range: 9.16 - 39.56\n",
      "  Standard deviation: 9.78\n",
      "  Change (1985-2015): +30.40\n",
      "\n",
      "2015 Risk Values:\n",
      "- CU: 0.11\n",
      "- FU: 0.00\n",
      "- PU: 39.45\n",
      "- COMB: 39.56\n",
      "\n",
      "Data Quality Checks:\n",
      "No data quality issues detected\n",
      "\n",
      " Cleaned data saved to data/processed/:\n",
      "    cu.csv\n",
      "    fu.csv\n",
      "    pu.csv\n",
      "    comb.csv\n",
      "\n",
      " Data structure summary:\n",
      "- Files created: 4\n",
      "- Time series length: 31 years\n",
      "- Year range: 1985 - 2015\n",
      "- Data types: {'year': dtype('int64'), 'yearName': dtype('int64'), 'cu': dtype('float64')}\n"
     ]
    }
   ],
   "source": [
    "# FLOODING - multiple csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_fu\"  / \"chart_fu\" (i.e., built-up area exposed to river (fluvial) flooding)\n",
    "# 2.) \"plot_pu\"  / \"chart_pu\" (i.e., built-up area exposed to rainwater (pluvial) flooding)\n",
    "# 3.) \"plot_cu\"  / \"chart_cu\" (i.e., built-up area exposed to coastal flooding)\n",
    "# 4.) \"plot_comb\" / \"chart_comb\" (i.e., built-up area exposed to combined flooding)\n",
    "#5.) Note: data, raw, \"flood-events.csv\" is already ready for Observable Notebook \"plot_fe\" / \"chart_fe\" (i.e.,large flood events in city, country)\n",
    "\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "# Note: flood data structure may vary by city - (i.e., coastal cities have coastal flood data, inland cities do not)\n",
    "raw_df_flood = pd.read_csv('data/raw/2025-11-mauritania-nouakchott_02-process-output_tabular_nouakchott_flood_wsf.csv') # updatefile path\n",
    "\n",
    "# basic info about raw data\n",
    "print(\"Raw flood risk data info:\")\n",
    "print(f\"Shape: {raw_df_flood.shape}\")\n",
    "print(f\"Columns: {list(raw_df_flood.columns)}\")\n",
    "print(f\"Year range: {raw_df_flood['year'].min()} - {raw_df_flood['year'].max()}\")\n",
    "print(f\"Total data points: {len(raw_df_flood)}\")\n",
    "\n",
    "# ID available flood types\n",
    "flood_columns = [col for col in raw_df_flood.columns if '_2020' in col]\n",
    "print(f\"Available flood types: {flood_columns}\")\n",
    "\n",
    "# preview flood risk ranges for each type\n",
    "for col in flood_columns:\n",
    "    flood_type = col.replace('_2020', '')\n",
    "    print(f\"{flood_type.capitalize()} risk range: {raw_df_flood[col].min():.2f} - {raw_df_flood[col].max():.2f}\")\n",
    "\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_flood.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean data using clean_flood function in clean.py\n",
    "try:\n",
    "    created_files = clean_flood('data/raw/2025-11-mauritania-nouakchott_02-process-output_tabular_nouakchott_flood_wsf.csv') # updatefile path\n",
    "    print(\"Flood risk data processed successfully!\")\n",
    "    \n",
    "    # load and validate each created file\n",
    "    flood_dataframes = {}\n",
    "    \n",
    "    for filename in created_files:\n",
    "        file_path = f'data/processed/{filename}'\n",
    "        flood_type = filename.replace('.csv', '')\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            flood_dataframes[flood_type] = df\n",
    "            \n",
    "            print(f\"\\n{filename} validation:\")\n",
    "            print(f\"- Shape: {df.shape}\")\n",
    "            print(f\"- Columns: {list(df.columns)}\")\n",
    "            print(f\"- Year range: {df['yearName'].min()} - {df['yearName'].max()}\")\n",
    "            print(f\"- Risk range: {df.iloc[:, 2].min():.2f} - {df.iloc[:, 2].max():.2f}\")  # Note: third column is risk value\n",
    "            print(f\"- Sample data:\")\n",
    "            print(df.head(5))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "    # basic flood risk analysis\n",
    "    if len(flood_dataframes) > 0:\n",
    "        print(f\"\\nFlood Risk Data Summary:\")\n",
    "        \n",
    "        for flood_type, df in flood_dataframes.items():\n",
    "            risk_column = df.columns[2]  # Note: third column contains risk values\n",
    "            \n",
    "            # basic statistics\n",
    "            avg_risk = df[risk_column].mean()\n",
    "            max_risk = df[risk_column].max()\n",
    "            min_risk = df[risk_column].min()\n",
    "            std_risk = df[risk_column].std()\n",
    "            \n",
    "            # trend calculation\n",
    "            trend = df[risk_column].iloc[-1] - df[risk_column].iloc[0]\n",
    "            \n",
    "            print(f\"\\n- {flood_type.upper()} flood risk:\")\n",
    "            print(f\"  Average: {avg_risk:.2f}\")\n",
    "            print(f\"  Range: {min_risk:.2f} - {max_risk:.2f}\")\n",
    "            print(f\"  Standard deviation: {std_risk:.2f}\")\n",
    "            print(f\"  Change (1985-2015): {trend:+.2f}\")\n",
    "        \n",
    "        # current values\n",
    "        if len(flood_dataframes) > 1:\n",
    "            print(f\"\\n2015 Risk Values:\")\n",
    "            for flood_type, df in flood_dataframes.items():\n",
    "                risk_column = df.columns[2]\n",
    "                current_risk = df[risk_column].iloc[-1]\n",
    "                print(f\"- {flood_type.upper()}: {current_risk:.2f}\")\n",
    "        \n",
    "        # data quality checks\n",
    "        print(f\"\\nData Quality Checks:\")\n",
    "        \n",
    "        quality_issues = 0\n",
    "        for flood_type, df in flood_dataframes.items():\n",
    "            risk_column = df.columns[2]\n",
    "            \n",
    "            # check for missing values\n",
    "            missing_values = df[risk_column].isna().sum()\n",
    "            if missing_values > 0:\n",
    "                print(f\"{flood_type.upper()}: {missing_values} missing values\")\n",
    "                quality_issues += 1\n",
    "            \n",
    "            # check for negative values (mathematically impossible for risk)\n",
    "            negative_values = (df[risk_column] < 0).sum()\n",
    "            if negative_values > 0:\n",
    "                print(f\"{flood_type.upper()}: {negative_values} negative risk values\")\n",
    "                quality_issues += 1\n",
    "        \n",
    "        if quality_issues == 0:\n",
    "            print(\"No data quality issues detected\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing flood risk data: {e}\")\n",
    "    print(\"Check that the flood CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: year, coastal_2020, fluvial_2020, pluvial_2020, comb_2020 (as available)\")\n",
    "\n",
    "# save confirmation and next steps\n",
    "if 'created_files' in locals() and len(created_files) > 0:\n",
    "    print(f\"\\n Cleaned data saved to data/processed/:\")\n",
    "    for filename in created_files:\n",
    "        print(f\"    {filename}\")\n",
    "    \n",
    "    # data structure summary\n",
    "    print(f\"\\n Data structure summary:\")\n",
    "    print(f\"- Files created: {len(created_files)}\")\n",
    "    print(f\"- Time series length: {len(list(flood_dataframes.values())[0]) if flood_dataframes else 'N/A'} years\")\n",
    "    print(f\"- Year range: {raw_df_flood['year'].min()} - {raw_df_flood['year'].max()}\")\n",
    "    print(f\"- Data types: {dict(list(flood_dataframes.values())[0].dtypes) if flood_dataframes else 'N/A'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No cleaned flood data available\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure flood CSV file exists in data/raw/\")\n",
    "    print(\"   2. Check file has correct columns with '_2020' suffix\")\n",
    "    print(\"   3. Verify data covers expected time range (1985-2015)\")\n",
    "    print(\"   4. Check that at least one flood type column exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566f301d",
   "metadata": {},
   "source": [
    "### ELEVATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f4cdf",
   "metadata": {},
   "source": [
    "### e.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_e\" / \"plot_e_alt\" / \"chart_e\" (i.e., elevation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe851ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw elevation data info:\n",
      "Shape: (5, 2)\n",
      "Columns: ['Bin', 'Count']\n",
      "Total data points: 5\n",
      "Elevation bins: 5\n",
      "Total pixels: 397,906\n",
      "Pixel count range: 682 - 307,332\n",
      "Elevation ranges: -6--1 to 14-19\n",
      "Data preview:\n",
      "     Bin   Count\n",
      "0  -6--1    4166\n",
      "1   -1-4  307332\n",
      "2    4-9   73591\n",
      "3   9-14   12135\n",
      "4  14-19     682\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/e.csv\n",
      "Elevation bins: 5\n",
      "Elevation range: 4-9 to -1-4\n",
      "Total area analyzed: 397,906 pixels\n",
      "Percentage coverage verification: 100.0% (should be ~100%)\n",
      "Dominant elevation range: -1-4 (77.2%)\n",
      "Major elevation ranges (≥10% coverage): 2 bins\n",
      "Major ranges: 4-9, -1-4\n",
      "Elevation data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (5, 3)\n",
      "Cleaned data columns: ['bin', 'count', 'percentage']\n",
      "Sample of cleaned data:\n",
      "     bin   count  percentage\n",
      "0    4-9   73591       18.49\n",
      "1   9-14   12135        3.05\n",
      "2  14-19     682        0.17\n",
      "3  -6--1    4166        1.05\n",
      "4   -1-4  307332       77.24\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 0\n",
      "- Elevation bins: 5\n",
      "- Total pixels: 397,906\n",
      "- Percentage sum: 100.0% (should be ~100%)\n",
      "\n",
      "Elevation Data Summary:\n",
      "- Elevation ranges present: 5\n",
      "- Pixel count range: 682 - 307,332\n",
      "- Percentage range: 0.17% - 77.24%\n",
      "- Most common elevation: -1-4 (77.24%)\n",
      "- Least common elevation: 14-19 (0.17%)\n",
      "- Lowest elevation range: 4-9\n",
      "- Highest elevation range: -1-4\n",
      "- Ranges with ≥20% coverage: 1\n",
      "- Ranges with ≥10% coverage: 2\n",
      "- Ranges with ≥5% coverage: 2\n",
      "\n",
      "Data Quality Checks:\n",
      "No data quality issues detected\n",
      "\n",
      "Cleaned data saved to: data/processed/e.csv\n",
      "\n",
      " Data structure summary:\n",
      "- Columns: ['bin', 'count', 'percentage']\n",
      "- Data points: 5 elevation ranges\n",
      "- Data types: {'bin': dtype('O'), 'count': dtype('int64'), 'percentage': dtype('float64')}\n",
      "- Elevation coverage: 0.17% - 77.24%\n"
     ]
    }
   ],
   "source": [
    "# ELEVATION - e.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_e\" / \"chart_e\" (elevation distribution vertical bar chart)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_e = pd.read_csv('data/raw/2025-11-mauritania-nouakchott_02-process-output_tabular_nouakchott_elevation.csv') # updatefile path\n",
    "\n",
    "# basic info about raw data\n",
    "print(\"Raw elevation data info:\")\n",
    "print(f\"Shape: {raw_df_e.shape}\")\n",
    "print(f\"Columns: {list(raw_df_e.columns)}\")\n",
    "print(f\"Total data points: {len(raw_df_e)}\")\n",
    "\n",
    "# preview elevation ranges and counts if available\n",
    "if 'Bin' in raw_df_e.columns and 'Count' in raw_df_e.columns:\n",
    "    print(f\"Elevation bins: {len(raw_df_e)}\")\n",
    "    print(f\"Total pixels: {raw_df_e['Count'].sum():,.0f}\")\n",
    "    print(f\"Pixel count range: {raw_df_e['Count'].min():,.0f} - {raw_df_e['Count'].max():,.0f}\")\n",
    "    print(f\"Elevation ranges: {raw_df_e['Bin'].iloc[0]} to {raw_df_e['Bin'].iloc[-1]}\")\n",
    "\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_e.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean data using clean_e function in clean.py\n",
    "try:\n",
    "    cleaned_df_e = clean_e('data/raw/2025-11-mauritania-nouakchott_02-process-output_tabular_nouakchott_elevation.csv') # updatefile path\n",
    "    print(\"Elevation data cleaned successfully!\")\n",
    "    \n",
    "    # cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_e.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_e.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_e.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_e.isnull().sum().sum()}\")\n",
    "    print(f\"- Elevation bins: {len(cleaned_df_e)}\")\n",
    "    print(f\"- Total pixels: {cleaned_df_e['count'].sum():,.0f}\")\n",
    "    print(f\"- Percentage sum: {cleaned_df_e['percentage'].sum():.1f}% (should be ~100%)\")\n",
    "    \n",
    "    # elevation data analysis\n",
    "    print(f\"\\nElevation Data Summary:\")\n",
    "    \n",
    "    # basic statistics\n",
    "    avg_percentage = cleaned_df_e['percentage'].mean()\n",
    "    median_percentage = cleaned_df_e['percentage'].median()\n",
    "    \n",
    "    print(f\"- Elevation ranges present: {len(cleaned_df_e)}\")\n",
    "    print(f\"- Pixel count range: {cleaned_df_e['count'].min():,.0f} - {cleaned_df_e['count'].max():,.0f}\")\n",
    "    print(f\"- Percentage range: {cleaned_df_e['percentage'].min():.2f}% - {cleaned_df_e['percentage'].max():.2f}%\")\n",
    "    \n",
    "    # ID extremes\n",
    "    dominant_bin = cleaned_df_e.loc[cleaned_df_e['percentage'].idxmax()]\n",
    "    least_common_bin = cleaned_df_e.loc[cleaned_df_e['percentage'].idxmin()]\n",
    "    \n",
    "    print(f\"- Most common elevation: {dominant_bin['bin']} ({dominant_bin['percentage']:.2f}%)\")\n",
    "    print(f\"- Least common elevation: {least_common_bin['bin']} ({least_common_bin['percentage']:.2f}%)\")\n",
    "    print(f\"- Lowest elevation range: {cleaned_df_e['bin'].iloc[0]}\")\n",
    "    print(f\"- Highest elevation range: {cleaned_df_e['bin'].iloc[-1]}\")\n",
    "    \n",
    "    # coverage distribution\n",
    "    above_20_percent = (cleaned_df_e['percentage'] >= 20).sum()\n",
    "    above_10_percent = (cleaned_df_e['percentage'] >= 10).sum()\n",
    "    above_5_percent = (cleaned_df_e['percentage'] >= 5).sum()\n",
    "    \n",
    "    print(f\"- Ranges with ≥20% coverage: {above_20_percent}\")\n",
    "    print(f\"- Ranges with ≥10% coverage: {above_10_percent}\")\n",
    "    print(f\"- Ranges with ≥5% coverage: {above_5_percent}\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values in key columns\n",
    "    missing_bin = cleaned_df_e['bin'].isna().sum()\n",
    "    missing_count = cleaned_df_e['count'].isna().sum()\n",
    "    missing_percentage = cleaned_df_e['percentage'].isna().sum()\n",
    "    \n",
    "    if missing_bin > 0:\n",
    "        print(f\"Missing elevation bin values: {missing_bin}\")\n",
    "        quality_issues += 1\n",
    "    if missing_count > 0:\n",
    "        print(f\"Missing count values: {missing_count}\")\n",
    "        quality_issues += 1\n",
    "    if missing_percentage > 0:\n",
    "        print(f\"Missing percentage values: {missing_percentage}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for impossible values\n",
    "    negative_count = (cleaned_df_e['count'] < 0).sum()\n",
    "    negative_percentage = (cleaned_df_e['percentage'] < 0).sum()\n",
    "    \n",
    "    if negative_count > 0:\n",
    "        print(f\"Negative count values: {negative_count}\")\n",
    "        quality_issues += 1\n",
    "    if negative_percentage > 0:\n",
    "        print(f\"Negative percentage values: {negative_percentage}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check percentage sum\n",
    "    percentage_sum = cleaned_df_e['percentage'].sum()\n",
    "    if abs(percentage_sum - 100) > 0.1:\n",
    "        print(f\"Percentage sum deviation: {percentage_sum:.1f}% (should be ~100%)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate elevation bins\n",
    "    duplicates = cleaned_df_e['bin'].duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Duplicate elevation bins: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"No data quality issues detected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error cleaning elevation data: {e}\")\n",
    "    print(\"Check that the elevation CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: Bin, Count\")\n",
    "\n",
    "# save confirmation and next steps\n",
    "if 'cleaned_df_e' in locals():\n",
    "    print(f\"\\nCleaned data saved to: data/processed/e.csv\")\n",
    "    \n",
    "    # preview of data structure\n",
    "    print(f\"\\n Data structure summary:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_e.columns)}\")\n",
    "    print(f\"- Data points: {len(cleaned_df_e)} elevation ranges\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_e.dtypes)}\")\n",
    "    print(f\"- Elevation coverage: {cleaned_df_e['percentage'].min():.2f}% - {cleaned_df_e['percentage'].max():.2f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"No cleaned elevation data available\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure elevation CSV file exists in data/raw/\")\n",
    "    print(\"   2. Check file has correct columns: Bin, Count\")\n",
    "    print(\"   3. Verify elevation bins are properly labeled\")\n",
    "    print(\"   4. Check that count values are numeric and positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593af0c8",
   "metadata": {},
   "source": [
    "### SLOPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca2b92",
   "metadata": {},
   "source": [
    "### s.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_s\" / \"chart_s\" (i.e., slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0121bfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw slope data info:\n",
      "Shape: (5, 2)\n",
      "Columns: ['Bin', 'Count']\n",
      "Total data points: 5\n",
      "Slope bins: 5\n",
      "Total pixels: 397,907\n",
      "Pixel count range: 0 - 388,800\n",
      "Slope ranges: 0-2 to 20-90 degrees\n",
      "Data preview:\n",
      "     Bin   Count\n",
      "0    0-2  388800\n",
      "1    2-5    8791\n",
      "2   5-10     308\n",
      "3  10-20       8\n",
      "4  20-90       0\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/s.csv\n",
      "Slope bins: 4\n",
      "Slope range: 0-2 to 10-20 degrees\n",
      "Total area analyzed: 397,907 pixels\n",
      "Percentage coverage verification: 100.0% (should be ~100%)\n",
      "Dominant slope range: 0-2 degrees (97.7%)\n",
      "Relatively flat areas (0-5 degrees): 97.7%\n",
      "Significant slope ranges (≥5% coverage): 1 bins\n",
      "Significant ranges: 0-2\n",
      "Slope data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (4, 3)\n",
      "Cleaned data columns: ['bin', 'count', 'percentage']\n",
      "Sample of cleaned data:\n",
      "     bin   count  percentage\n",
      "0    0-2  388800       97.71\n",
      "1    2-5    8791        2.21\n",
      "2   5-10     308        0.08\n",
      "3  10-20       8        0.00\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 0\n",
      "- Slope bins: 4\n",
      "- Total pixels: 397,907\n",
      "- Percentage sum: 100.0% (should be ~100%)\n",
      "\n",
      "Slope Data Summary:\n",
      "- Slope ranges present: 4\n",
      "- Pixel count range: 8 - 388,800\n",
      "- Percentage range: 0.00% - 97.71%\n",
      "- Most common slope: 0-2 degrees (97.71%)\n",
      "- Least common slope: 10-20 degrees (0.00%)\n",
      "- Gentlest slope range: 0-2 degrees\n",
      "- Steepest slope range: 10-20 degrees\n",
      "- Ranges with ≥20% coverage: 1\n",
      "- Ranges with ≥10% coverage: 1\n",
      "- Ranges with ≥5% coverage: 1\n",
      "\n",
      "Data Quality Checks:\n",
      "No data quality issues detected\n",
      "\n",
      "Cleaned data saved to: data/processed/s.csv\n",
      "\n",
      "Data structure summary:\n",
      "- Columns: ['bin', 'count', 'percentage']\n",
      "- Data points: 4 slope ranges\n",
      "- Data types: {'bin': dtype('O'), 'count': dtype('int64'), 'percentage': dtype('float64')}\n",
      "- Slope coverage: 0.00% - 97.71%\n"
     ]
    }
   ],
   "source": [
    "# SLOPE - s.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_s\" / \"chart_s\" (slope chart)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_s = pd.read_csv('data/raw/2025-11-mauritania-nouakchott_02-process-output_tabular_nouakchott_slope.csv') # updatefile path\n",
    "\n",
    "# basic info about raw data\n",
    "print(\"Raw slope data info:\")\n",
    "print(f\"Shape: {raw_df_s.shape}\")\n",
    "print(f\"Columns: {list(raw_df_s.columns)}\")\n",
    "print(f\"Total data points: {len(raw_df_s)}\")\n",
    "\n",
    "# slope ranges and counts\n",
    "if 'Bin' in raw_df_s.columns and 'Count' in raw_df_s.columns:\n",
    "    print(f\"Slope bins: {len(raw_df_s)}\")\n",
    "    print(f\"Total pixels: {raw_df_s['Count'].sum():,.0f}\")\n",
    "    print(f\"Pixel count range: {raw_df_s['Count'].min():,.0f} - {raw_df_s['Count'].max():,.0f}\")\n",
    "    print(f\"Slope ranges: {raw_df_s['Bin'].iloc[0]} to {raw_df_s['Bin'].iloc[-1]} degrees\")\n",
    "\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_s.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean data using clean_s function in clean.py\n",
    "try:\n",
    "    cleaned_df_s = clean_s('data/raw/2025-11-mauritania-nouakchott_02-process-output_tabular_nouakchott_slope.csv') # updatefile path\n",
    "    print(\"Slope data cleaned successfully!\")\n",
    "    \n",
    "    # cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_s.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_s.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_s.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_s.isnull().sum().sum()}\")\n",
    "    print(f\"- Slope bins: {len(cleaned_df_s)}\")\n",
    "    print(f\"- Total pixels: {cleaned_df_s['count'].sum():,.0f}\")\n",
    "    print(f\"- Percentage sum: {cleaned_df_s['percentage'].sum():.1f}% (should be ~100%)\")\n",
    "    \n",
    "    # slope data analysis\n",
    "    print(f\"\\nSlope Data Summary:\")\n",
    "    \n",
    "    print(f\"- Slope ranges present: {len(cleaned_df_s)}\")\n",
    "    print(f\"- Pixel count range: {cleaned_df_s['count'].min():,.0f} - {cleaned_df_s['count'].max():,.0f}\")\n",
    "    print(f\"- Percentage range: {cleaned_df_s['percentage'].min():.2f}% - {cleaned_df_s['percentage'].max():.2f}%\")\n",
    "    \n",
    "    # ID extremes\n",
    "    dominant_bin = cleaned_df_s.loc[cleaned_df_s['percentage'].idxmax()]\n",
    "    least_common_bin = cleaned_df_s.loc[cleaned_df_s['percentage'].idxmin()]\n",
    "    \n",
    "    print(f\"- Most common slope: {dominant_bin['bin']} degrees ({dominant_bin['percentage']:.2f}%)\")\n",
    "    print(f\"- Least common slope: {least_common_bin['bin']} degrees ({least_common_bin['percentage']:.2f}%)\")\n",
    "    print(f\"- Gentlest slope range: {cleaned_df_s['bin'].iloc[0]} degrees\")\n",
    "    print(f\"- Steepest slope range: {cleaned_df_s['bin'].iloc[-1]} degrees\")\n",
    "    \n",
    "    # coverage distribution\n",
    "    above_20_percent = (cleaned_df_s['percentage'] >= 20).sum()\n",
    "    above_10_percent = (cleaned_df_s['percentage'] >= 10).sum()\n",
    "    above_5_percent = (cleaned_df_s['percentage'] >= 5).sum()\n",
    "    \n",
    "    print(f\"- Ranges with ≥20% coverage: {above_20_percent}\")\n",
    "    print(f\"- Ranges with ≥10% coverage: {above_10_percent}\")\n",
    "    print(f\"- Ranges with ≥5% coverage: {above_5_percent}\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values in key columns\n",
    "    missing_bin = cleaned_df_s['bin'].isna().sum()\n",
    "    missing_count = cleaned_df_s['count'].isna().sum()\n",
    "    missing_percentage = cleaned_df_s['percentage'].isna().sum()\n",
    "    \n",
    "    if missing_bin > 0:\n",
    "        print(f\"Missing slope bin values: {missing_bin}\")\n",
    "        quality_issues += 1\n",
    "    if missing_count > 0:\n",
    "        print(f\"Missing count values: {missing_count}\")\n",
    "        quality_issues += 1\n",
    "    if missing_percentage > 0:\n",
    "        print(f\"Missing percentage values: {missing_percentage}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for impossible values\n",
    "    negative_count = (cleaned_df_s['count'] < 0).sum()\n",
    "    negative_percentage = (cleaned_df_s['percentage'] < 0).sum()\n",
    "    \n",
    "    if negative_count > 0:\n",
    "        print(f\"Negative count values: {negative_count}\")\n",
    "        quality_issues += 1\n",
    "    if negative_percentage > 0:\n",
    "        print(f\"Negative percentage values: {negative_percentage}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check percentage sum\n",
    "    percentage_sum = cleaned_df_s['percentage'].sum()\n",
    "    if abs(percentage_sum - 100) > 0.1:\n",
    "        print(f\"Percentage sum deviation: {percentage_sum:.1f}% (should be ~100%)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate slope bins\n",
    "    duplicates = cleaned_df_s['bin'].duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Duplicate slope bins: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"No data quality issues detected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error cleaning slope data: {e}\")\n",
    "    print(\"Check that the slope CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: Bin, Count\")\n",
    "\n",
    "# save confirmation and next steps\n",
    "if 'cleaned_df_s' in locals():\n",
    "    print(f\"\\nCleaned data saved to: data/processed/s.csv\")\n",
    "    \n",
    "    # quick preview of data structure\n",
    "    print(f\"\\nData structure summary:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_s.columns)}\")\n",
    "    print(f\"- Data points: {len(cleaned_df_s)} slope ranges\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_s.dtypes)}\")\n",
    "    print(f\"- Slope coverage: {cleaned_df_s['percentage'].min():.2f}% - {cleaned_df_s['percentage'].max():.2f}%\")\n",
    "      \n",
    "else:\n",
    "    print(\"No cleaned slope data available\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure slope CSV file exists in data/raw/\")\n",
    "    print(\"   2. Check file has correct columns: Bin, Count\")\n",
    "    print(\"   3. Verify slope bins are properly labeled (e.g., 0-2, 2-5)\")\n",
    "    print(\"   4. Check that count values are numeric and positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c13695e",
   "metadata": {},
   "source": [
    "### LANDSLIDE SUSCEPTIBILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54789ea0",
   "metadata": {},
   "source": [
    "### ls_area.csv preparation (i.e., % area with different landslide susceptibility levels - \"No Data\" (0); \"Very low\" (1); \"Low\" (2); \"Medium\" (3); \"High\" (4); and \"Very high\" (5))\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_ls_area\" / \"chart_ls_area\" (i.e., Percentage of area with different landslide susceptibiltiy levels, \"No Data\" (0); \"Very low\" (1); \"Low\" (2); \"Medium\" (3); \"High\" (4); and \"Very high\" (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3e819b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LANDSLIDE SUSCEPTIBILITY DATA PROCESSING\n",
      "============================================================\n",
      "Analyzing TIF data structure...\n",
      "Unique values in TIF: [  0   1   2 127]\n",
      "Data range: 0 to 127\n",
      "NoData value: 127.0\n",
      "Value 0: 59 pixels\n",
      "Value 1: 423 pixels\n",
      "Value 2: 9 pixels\n",
      "Value 127: 9 pixels\n",
      "\n",
      "----------------------------------------\n",
      "Unique values found in data: [1 2]\n",
      "Cleaned landslide data saved to: data/processed/ls_area.csv\n",
      "Susceptibility categories: 5\n",
      "Total pixels analyzed: 432\n",
      "Percentage coverage verification: 100.0% (should be ~100%)\n",
      "Dominant susceptibility level: Very low (97.9%)\n",
      "Landslide susceptibility data processed successfully!\n",
      "\n",
      "Cleaned data shape: (5, 4)\n",
      "Cleaned data columns: ['bin', 'susceptibility', 'count', 'percentage']\n",
      "\n",
      "Processed Landslide Susceptibility data:\n",
      "         bin susceptibility  count  percentage\n",
      "0   Very low              1    423       97.92\n",
      "1        Low              2      9        2.08\n",
      "2     Medium              3      0        0.00\n",
      "3       High              4      0        0.00\n",
      "4  Very high              5      0        0.00\n",
      "\n",
      "Data Validation:\n",
      "- Missing values: 0\n",
      "- Susceptibility categories: 5\n",
      "- Total pixels: 432\n",
      "- Percentage sum: 100.0% (should be ~100%)\n",
      "\n",
      "Landslide Susceptibility Distribution:\n",
      "- Very low: 423 pixels (97.9%)\n",
      "- Low: 9 pixels (2.1%)\n",
      "\n",
      "- Most common susceptibility: Very low (97.9%)\n",
      "- Least common susceptibility: Low (2.1%)\n",
      "\n",
      "Risk Level Summary:\n",
      "- Very low risk areas: 97.9%\n",
      "- Low risk areas: 2.1%\n",
      "- Medium risk areas: 0.0%\n",
      "- High risk areas: 0.0%\n",
      "- Very high risk areas: 0.0%\n",
      "\n",
      "Data Quality Checks:\n",
      "No data quality issues detected\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# LANDSLIDE SUSCEPTIBILITY - ls_area.csv preparation from raw tif data for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_ls_area\" / \"chart_ls_area\" (i.e., Percentage of Area with different landslide susceptibility levels, \"No Data\" (0); \"Very low\" (1); \"Low\" (2); \"Medium\" (3); \"High\" (4); and \"Very high\" (5)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tif data\n",
    "input_tif_path = 'data/raw/2025-11-mauritania-nouakchott_02-process-output_spatial_nouakchott_landslide.tif'  # updatefile path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LANDSLIDE SUSCEPTIBILITY DATA PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# data value distribution\n",
    "print(\"Analyzing TIF data structure...\")\n",
    "\n",
    "try:\n",
    "    import rasterio\n",
    "    with rasterio.open(input_tif_path) as src:\n",
    "        data = src.read(1)\n",
    "        unique_vals = np.unique(data[~np.isnan(data)])\n",
    "        print(f\"Unique values in TIF: {unique_vals}\")\n",
    "        print(f\"Data range: {data.min()} to {data.max()}\")\n",
    "        print(f\"NoData value: {src.nodata}\")\n",
    "        \n",
    "        # count pixels for each value\n",
    "        for val in unique_vals:\n",
    "            count = np.sum(data == val)\n",
    "            print(f\"Value {val}: {count:,} pixels\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not examine TIF structure: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "\n",
    "# process tif file using clean_ls_area function in clean.py\n",
    "# set \"include_nodata=False\" to exclude value, \"0\"  (\"NoData\"/background)\n",
    "try:\n",
    "    cleaned_df_landslide = clean_ls_area(input_tif_path, include_nodata=False)\n",
    "    print(\"Landslide susceptibility data processed successfully!\")\n",
    "    \n",
    "    # cleaned data structure\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_landslide.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_landslide.columns)}\")\n",
    "    print(f\"\\nProcessed Landslide Susceptibility data:\")\n",
    "    print(cleaned_df_landslide)\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData Validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_landslide.isnull().sum().sum()}\")\n",
    "    print(f\"- Susceptibility categories: {len(cleaned_df_landslide)}\")\n",
    "    print(f\"- Total pixels: {cleaned_df_landslide['count'].sum():,.0f}\")\n",
    "    print(f\"- Percentage sum: {cleaned_df_landslide['percentage'].sum():.1f}% (should be ~100%)\")\n",
    "    \n",
    "    # landslide susceptibility distribution analysis\n",
    "    print(f\"\\nLandslide Susceptibility Distribution:\")\n",
    "    \n",
    "    total_pixels = cleaned_df_landslide['count'].sum()\n",
    "    \n",
    "    for idx, row in cleaned_df_landslide.iterrows():\n",
    "        if row['count'] > 0:  # only show categories with data (i.e., exclude \"0\")\n",
    "            print(f\"- {row['bin']}: {row['count']:,.0f} pixels ({row['percentage']:.1f}%)\")\n",
    "    \n",
    "    # risk level analysis\n",
    "    if len(cleaned_df_landslide) > 0:\n",
    "        # filter out zero-count categories\n",
    "        active_categories = cleaned_df_landslide[cleaned_df_landslide['count'] > 0]\n",
    "        \n",
    "        if len(active_categories) > 0:\n",
    "            max_susceptibility = active_categories.loc[active_categories['percentage'].idxmax()]\n",
    "            min_susceptibility = active_categories.loc[active_categories['percentage'].idxmin()]\n",
    "            \n",
    "            print(f\"\\n- Most common susceptibility: {max_susceptibility['bin']} ({max_susceptibility['percentage']:.1f}%)\")\n",
    "            print(f\"- Least common susceptibility: {min_susceptibility['bin']} ({min_susceptibility['percentage']:.1f}%)\")\n",
    "    \n",
    "    # risk level groupings\n",
    "    very_low_risk = cleaned_df_landslide[cleaned_df_landslide['bin'].isin(['Very low'])]['percentage'].sum()\n",
    "    low_risk = cleaned_df_landslide[cleaned_df_landslide['bin'].isin(['Low'])]['percentage'].sum()\n",
    "    medium_risk = cleaned_df_landslide[cleaned_df_landslide['bin'] == 'Medium']['percentage'].sum()\n",
    "    high_risk = cleaned_df_landslide[cleaned_df_landslide['bin'].isin(['High'])]['percentage'].sum()\n",
    "    very_high_risk = cleaned_df_landslide[cleaned_df_landslide['bin'].isin(['Very high'])]['percentage'].sum()\n",
    "    \n",
    "    print(f\"\\nRisk Level Summary:\")\n",
    "    print(f\"- Very low risk areas: {very_low_risk:.1f}%\")\n",
    "    print(f\"- Low risk areas: {low_risk:.1f}%\")\n",
    "    print(f\"- Medium risk areas: {medium_risk:.1f}%\")\n",
    "    print(f\"- High risk areas: {very_high_risk:.1f}%\")\n",
    "    print(f\"- Very high risk areas: {high_risk:.1f}%\")\n",
    "\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values\n",
    "    missing_values = cleaned_df_landslide.isnull().sum().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"Missing values detected: {missing_values}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for negative values (should not exist)\n",
    "    negative_counts = (cleaned_df_landslide['count'] < 0).sum()\n",
    "    negative_percentages = (cleaned_df_landslide['percentage'] < 0).sum()\n",
    "    \n",
    "    if negative_counts > 0:\n",
    "        print(f\"Negative count values: {negative_counts}\")\n",
    "        quality_issues += 1\n",
    "    if negative_percentages > 0:\n",
    "        print(f\"Negative percentage values: {negative_percentages}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check percentage sum\n",
    "    percentage_sum = cleaned_df_landslide['percentage'].sum()\n",
    "    if abs(percentage_sum - 100) > 0.1:\n",
    "        print(f\"Percentage sum deviation: {percentage_sum:.1f}% (should be ~100%)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate categories\n",
    "    duplicates = cleaned_df_landslide['bin'].duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Duplicate susceptibility categories: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for expected landslide susceptibility categories\n",
    "    expected_categories = ['Very low', 'Low', 'Medium', 'High', 'Very high']\n",
    "    actual_categories = cleaned_df_landslide['bin'].tolist()\n",
    "    missing_categories = set(expected_categories) - set(actual_categories)\n",
    "    if missing_categories:\n",
    "        print(f\"Categories not present in data: {missing_categories}\")\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"No data quality issues detected\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing landslide susceptibility data: {e}\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure TIF file exists at the specified path\")\n",
    "    print(\"   2. Check that the TIF file contains values 1-5 (or 0-5)\")\n",
    "    print(\"   3. Verify the TIF file is not corrupted\")\n",
    "    print(\"   4. Ensure rasterio library is installed: pip install rasterio\")\n",
    "    print(\"   5. Check file permissions and disk space\")\n",
    "    print(\"   6. If data includes value 0, try setting include_nodata=True\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# optional: to include value, \"0\" in analysis\n",
    "# print(\"\\n\" + \"=\"*30 + \" INCLUDING VALUE 0 \" + \"=\"*30)\n",
    "# try:\n",
    "#     cleaned_df_with_zero = clean_ls_area(input_tif_path, \n",
    "#                                                output_file='data/processed/ls_area_with_nodata.csv',\n",
    "#                                                include_nodata=True)\n",
    "#     print(\"Alternative analysis (including value, \"0\") completed!\")\n",
    "#     print(cleaned_df_with_zero)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error in alternative analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3867ed",
   "metadata": {},
   "source": [
    "### EARTHQUAKE EVENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927e645",
   "metadata": {},
   "source": [
    "### ee.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_ee\" / \"chart_ee\" (i.e., Significant earthquakes within 500 km since 1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888896be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EARTHQUAKE EVENTS - ee.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_ee\" / \"chart_ee\" (Significant Earthquakes within 500 km since 1900)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_ee = pd.read_csv('data/raw/earthquake-events.csv') # updatefile path\n",
    "\n",
    "# basic info about raw data\n",
    "print(\"Raw earthquake events data info:\")\n",
    "print(f\"Shape: {raw_df_ee.shape}\")\n",
    "print(f\"Columns: {list(raw_df_ee.columns)}\")\n",
    "print(f\"Total data points: {len(raw_df_ee)}\")\n",
    "\n",
    "# preview key data ranges if available\n",
    "if 'eqMagnitude' in raw_df_ee.columns:\n",
    "    print(f\"Magnitude range: {raw_df_ee['eqMagnitude'].min():.1f} - {raw_df_ee['eqMagnitude'].max():.1f}\")\n",
    "if 'distance' in raw_df_ee.columns:\n",
    "    print(f\"Distance range: {raw_df_ee['distance'].min():.0f} - {raw_df_ee['distance'].max():.0f} km\")\n",
    "\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_ee.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean data using clean_ee function in clean.py\n",
    "try:\n",
    "    cleaned_df_ee = clean_ee('data/raw/earthquake-events.csv') # updatefile path\n",
    "    print(\"Earthquake events data cleaned successfully!\")\n",
    "    \n",
    "    # cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_ee.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_ee.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_ee.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_ee.isnull().sum().sum()}\")\n",
    "    print(f\"- Year range: {cleaned_df_ee['begin_year'].min()} - {cleaned_df_ee['begin_year'].max()}\")\n",
    "    print(f\"- Magnitude range: {cleaned_df_ee['eqMagnitude'].min():.1f} - {cleaned_df_ee['eqMagnitude'].max():.1f}\")\n",
    "    print(f\"- Distance range: {cleaned_df_ee['distance'].min():.0f} - {cleaned_df_ee['distance'].max():.0f} km\")\n",
    "    \n",
    "    # earthquake data analysis\n",
    "    print(f\"\\nEarthquake Data Summary:\")\n",
    "    \n",
    "    # basic statistics\n",
    "    avg_magnitude = cleaned_df_ee['eqMagnitude'].mean()\n",
    "    avg_distance = cleaned_df_ee['distance'].mean()\n",
    "    \n",
    "    print(f\"- Total events: {len(cleaned_df_ee)}\")\n",
    "    print(f\"- Average magnitude: {avg_magnitude:.1f}\")\n",
    "    print(f\"- Average distance: {avg_distance:.0f} km\")\n",
    "    print(f\"- Standard deviation (magnitude): {cleaned_df_ee['eqMagnitude'].std():.1f}\")\n",
    "    print(f\"- Standard deviation (distance): {cleaned_df_ee['distance'].std():.0f} km\")\n",
    "    \n",
    "    # temporal distribution\n",
    "    years_span = cleaned_df_ee['begin_year'].max() - cleaned_df_ee['begin_year'].min()\n",
    "    events_per_decade = (len(cleaned_df_ee) / years_span) * 10 if years_span > 0 else 0\n",
    "    \n",
    "    print(f\"- Time span: {years_span} years\")\n",
    "    print(f\"- Average events per decade: {events_per_decade:.1f}\")\n",
    "    \n",
    "    # ID extremes\n",
    "    strongest_eq = cleaned_df_ee.loc[cleaned_df_ee['eqMagnitude'].idxmax()]\n",
    "    closest_eq = cleaned_df_ee.loc[cleaned_df_ee['distance'].idxmin()]\n",
    "    \n",
    "    print(f\"- Strongest earthquake: {strongest_eq['eqMagnitude']:.1f} magnitude in {strongest_eq['begin_year']}\")\n",
    "    print(f\"- Closest earthquake: {closest_eq['distance']:.0f} km in {closest_eq['begin_year']}\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values in key columns\n",
    "    missing_magnitude = cleaned_df_ee['eqMagnitude'].isna().sum()\n",
    "    missing_distance = cleaned_df_ee['distance'].isna().sum()\n",
    "    missing_year = cleaned_df_ee['begin_year'].isna().sum()\n",
    "    \n",
    "    if missing_magnitude > 0:\n",
    "        print(f\"Missing magnitude values: {missing_magnitude}\")\n",
    "        quality_issues += 1\n",
    "    if missing_distance > 0:\n",
    "        print(f\"Missing distance values: {missing_distance}\")\n",
    "        quality_issues += 1\n",
    "    if missing_year > 0:\n",
    "        print(f\"Missing year values: {missing_year}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for impossible values\n",
    "    negative_magnitude = (cleaned_df_ee['eqMagnitude'] < 0).sum()\n",
    "    negative_distance = (cleaned_df_ee['distance'] < 0).sum()\n",
    "    \n",
    "    if negative_magnitude > 0:\n",
    "        print(f\"Negative magnitude values: {negative_magnitude}\")\n",
    "        quality_issues += 1\n",
    "    if negative_distance > 0:\n",
    "        print(f\"Negative distance values: {negative_distance}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate events (same year, magnitude, and distance)\n",
    "    duplicates = cleaned_df_ee.duplicated(subset=['begin_year', 'eqMagnitude', 'distance']).sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Potential duplicate events: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"No data quality issues detected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error cleaning earthquake events data: {e}\")\n",
    "    print(\"Check that the earthquake-events.csv file exists and has the correct format\")\n",
    "    print(\"Expected columns: BEGAN, eqMagnitude, distance, text, line1, line2, line3\")\n",
    "\n",
    "# save confirmation and next steps\n",
    "if 'cleaned_df_ee' in locals():\n",
    "    print(f\"\\nCleaned data saved to: data/processed/ee.csv\")\n",
    "    \n",
    "    # preview of data structure\n",
    "    print(f\"\\nData structure summary:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_ee.columns)}\")\n",
    "    print(f\"- Time series length: {len(cleaned_df_ee)} events\")\n",
    "    print(f\"- Year range: {cleaned_df_ee['begin_year'].min()} - {cleaned_df_ee['begin_year'].max()}\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_ee.dtypes)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No cleaned earthquake data available\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure earthquake-events.csv exists in data/raw/\")\n",
    "    print(\"   2. Check file has correct columns: BEGAN, eqMagnitude, distance, text, line1, line2, line3\")\n",
    "    print(\"   3. Verify data contains parseable dates in BEGAN column\")\n",
    "    print(\"   4. Check that magnitude and distance values are numeric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d153b3",
   "metadata": {},
   "source": [
    "### LIQUEFACTION SUSCEPTIBILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2221b45",
   "metadata": {},
   "source": [
    "### l_area.csv preparation (i.e., % area with different liquefaction susceptibility levels - \"No Data\" (0); \"Very low\" (1); \"Low\" (2); \"Medium\" (3); \"High\" (4); and \"Very high\" (5))\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_l_area\" / \"chart_l_area\" (i.e., Percentage of area with different liquefaction susceptibiltiy levels, \"No Data\" (0); \"Very low\" (1); \"Low\" (2); \"Medium\" (3); \"High\" (4); and \"Very high\" (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e86b619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LIQUEFACTION SUSCEPTIBILITY DATA PROCESSING\n",
      "============================================================\n",
      "Analyzing TIF data structure...\n",
      "Unique values in TIF: [  0   3   4 255]\n",
      "Data range: 0 to 255\n",
      "NoData value: 255.0\n",
      "Value 0: 42 pixels\n",
      "Value 3: 121 pixels\n",
      "Value 4: 158 pixels\n",
      "Value 255: 15 pixels\n",
      "\n",
      "----------------------------------------\n",
      "Unique values found in data: [3 4]\n",
      "Cleaned liquefaction data saved to: data/processed/l_area.csv\n",
      "Susceptibility categories: 5\n",
      "Total pixels analyzed: 279\n",
      "Percentage coverage verification: 100.0% (should be ~100%)\n",
      "Dominant susceptibility level: High (56.6%)\n",
      "Liquefaction susceptibility data processed successfully!\n",
      "\n",
      "Cleaned data shape: (5, 4)\n",
      "Cleaned data columns: ['bin', 'susceptibility', 'count', 'percentage']\n",
      "\n",
      "Processed Liquefaction Susceptibility data:\n",
      "         bin susceptibility  count  percentage\n",
      "0   Very low              1      0        0.00\n",
      "1        Low              2      0        0.00\n",
      "2     Medium              3    121       43.37\n",
      "3       High              4    158       56.63\n",
      "4  Very high              5      0        0.00\n",
      "\n",
      "Data Validation:\n",
      "- Missing values: 0\n",
      "- Susceptibility categories: 5\n",
      "- Total pixels: 279\n",
      "- Percentage sum: 100.0% (should be ~100%)\n",
      "\n",
      "Liquefaction Susceptibility Distribution:\n",
      "- Medium: 121 pixels (43.4%)\n",
      "- High: 158 pixels (56.6%)\n",
      "\n",
      "- Most common susceptibility: High (56.6%)\n",
      "- Least common susceptibility: Medium (43.4%)\n",
      "\n",
      "Risk Level Summary:\n",
      "- Very low risk areas: 0.0%\n",
      "- Low risk areas: 0.0%\n",
      "- Medium risk areas: 43.4%\n",
      "- High risk areas: 56.6%\n",
      "- Very high risk areas: 0.0%\n",
      "\n",
      "Data Quality Checks:\n",
      "No data quality issues detected\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# LIQUEFACTION SUSCEPTIBILITY - l_area.csv preparation from raw tif data for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_l_area\" / \"chart_l_area\" (i.e., Percentage of Area with different liquefaction susceptibility levels, \"No Data\" (0); \"Very low\" (1); \"Low\" (2); \"Medium\" (3); \"High\" (4); and \"Very high\" (5)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tif data\n",
    "input_tif_path = 'data/raw/2025-11-mauritania-nouakchott_02-process-output_spatial_nouakchott_liquefaction.tif' # updatefile path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LIQUEFACTION SUSCEPTIBILITY DATA PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# data value distribution\n",
    "print(\"Analyzing TIF data structure...\")\n",
    "\n",
    "try:\n",
    "    import rasterio\n",
    "    with rasterio.open(input_tif_path) as src:\n",
    "        data = src.read(1)\n",
    "        unique_vals = np.unique(data[~np.isnan(data)])\n",
    "        print(f\"Unique values in TIF: {unique_vals}\")\n",
    "        print(f\"Data range: {data.min()} to {data.max()}\")\n",
    "        print(f\"NoData value: {src.nodata}\")\n",
    "        \n",
    "        # count pixels for each value\n",
    "        for val in unique_vals:\n",
    "            count = np.sum(data == val)\n",
    "            print(f\"Value {val}: {count:,} pixels\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not examine TIF structure: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "\n",
    "# process tif file using clean_l_area function in clean.py\n",
    "# set \"include_nodata=False\" to exclude value, \"0\" (\"NoData\"/background)\n",
    "try:\n",
    "    cleaned_df_liquefaction = clean_l_area(input_tif_path, include_nodata=False)\n",
    "    print(\"Liquefaction susceptibility data processed successfully!\")\n",
    "    \n",
    "    # cleaned data structure\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_liquefaction.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_liquefaction.columns)}\")\n",
    "    print(f\"\\nProcessed Liquefaction Susceptibility data:\")\n",
    "    print(cleaned_df_liquefaction)\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData Validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_liquefaction.isnull().sum().sum()}\")\n",
    "    print(f\"- Susceptibility categories: {len(cleaned_df_liquefaction)}\")\n",
    "    print(f\"- Total pixels: {cleaned_df_liquefaction['count'].sum():,.0f}\")\n",
    "    print(f\"- Percentage sum: {cleaned_df_liquefaction['percentage'].sum():.1f}% (should be ~100%)\")\n",
    "    \n",
    "    # liquefaction susceptibility distribution analysis\n",
    "    print(f\"\\nLiquefaction Susceptibility Distribution:\")\n",
    "    \n",
    "    total_pixels = cleaned_df_liquefaction['count'].sum()\n",
    "    \n",
    "    for idx, row in cleaned_df_liquefaction.iterrows():\n",
    "        if row['count'] > 0:  # only show categories with data\n",
    "            print(f\"- {row['bin']}: {row['count']:,.0f} pixels ({row['percentage']:.1f}%)\")\n",
    "    \n",
    "    # risk level analysis\n",
    "    if len(cleaned_df_liquefaction) > 0:\n",
    "        # filter out zero-count categories\n",
    "        active_categories = cleaned_df_liquefaction[cleaned_df_liquefaction['count'] > 0]\n",
    "        \n",
    "        if len(active_categories) > 0:\n",
    "            max_susceptibility = active_categories.loc[active_categories['percentage'].idxmax()]\n",
    "            min_susceptibility = active_categories.loc[active_categories['percentage'].idxmin()]\n",
    "            \n",
    "            print(f\"\\n- Most common susceptibility: {max_susceptibility['bin']} ({max_susceptibility['percentage']:.1f}%)\")\n",
    "            print(f\"- Least common susceptibility: {min_susceptibility['bin']} ({min_susceptibility['percentage']:.1f}%)\")\n",
    "    \n",
    "    # risk level groupings\n",
    "    very_low_risk = cleaned_df_liquefaction[cleaned_df_liquefaction['bin'].isin(['Very low'])]['percentage'].sum()\n",
    "    low_risk = cleaned_df_liquefaction[cleaned_df_liquefaction['bin'].isin(['Low'])]['percentage'].sum()\n",
    "    medium_risk = cleaned_df_liquefaction[cleaned_df_liquefaction['bin'] == 'Medium']['percentage'].sum()\n",
    "    high_risk = cleaned_df_liquefaction[cleaned_df_liquefaction['bin'].isin(['High'])]['percentage'].sum()\n",
    "    very_high_risk = cleaned_df_liquefaction[cleaned_df_liquefaction['bin'].isin(['Very high'])]['percentage'].sum()\n",
    "\n",
    "    \n",
    "    print(f\"\\nRisk Level Summary:\")\n",
    "    print(f\"- Very low risk areas: {very_low_risk:.1f}%\")\n",
    "    print(f\"- Low risk areas: {low_risk:.1f}%\")\n",
    "    print(f\"- Medium risk areas: {medium_risk:.1f}%\")\n",
    "    print(f\"- High risk areas: {high_risk:.1f}%\")\n",
    "    print(f\"- Very high risk areas: {very_high_risk:.1f}%\")\n",
    "\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values\n",
    "    missing_values = cleaned_df_liquefaction.isnull().sum().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"Missing values detected: {missing_values}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for negative values (should not exist)\n",
    "    negative_counts = (cleaned_df_liquefaction['count'] < 0).sum()\n",
    "    negative_percentages = (cleaned_df_liquefaction['percentage'] < 0).sum()\n",
    "    \n",
    "    if negative_counts > 0:\n",
    "        print(f\"Negative count values: {negative_counts}\")\n",
    "        quality_issues += 1\n",
    "    if negative_percentages > 0:\n",
    "        print(f\"Negative percentage values: {negative_percentages}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check percentage sum\n",
    "    percentage_sum = cleaned_df_liquefaction['percentage'].sum()\n",
    "    if abs(percentage_sum - 100) > 0.1:\n",
    "        print(f\"Percentage sum deviation: {percentage_sum:.1f}% (should be ~100%)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate categories\n",
    "    duplicates = cleaned_df_liquefaction['bin'].duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Duplicate susceptibility categories: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for expected liquefaction susceptibility categories\n",
    "    expected_categories = ['Very low', 'Low', 'Medium', 'High', 'Very high']\n",
    "    actual_categories = cleaned_df_liquefaction['bin'].tolist()\n",
    "    missing_categories = set(expected_categories) - set(actual_categories)\n",
    "    if missing_categories:\n",
    "        print(f\"Categories not present in data: {missing_categories}\")\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"No data quality issues detected\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing liquefaction susceptibility data: {e}\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure TIF file exists at the specified path\")\n",
    "    print(\"   2. Check that the TIF file contains values 1-5 (or 0-5)\")\n",
    "    print(\"   3. Verify the TIF file is not corrupted\")\n",
    "    print(\"   4. Ensure rasterio library is installed: pip install rasterio\")\n",
    "    print(\"   5. Check file permissions and disk space\")\n",
    "    print(\"   6. If data includes value 0, try setting include_nodata=True\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# optional: to include value, \"0\" in analysis\n",
    "# print(\"\\n\" + \"=\"*30 + \" INCLUDING VALUE 0 \" + \"=\"*30)\n",
    "# try:\n",
    "#     cleaned_df_with_zero = clean_liquefaction_area(input_tif_path, \n",
    "#                                                   output_file='data/processed/l_area_with_nodata.csv',\n",
    "#                                                   include_nodata=True)\n",
    "#     print(\"Alternative analysis (including value 0) completed!\")\n",
    "#     print(cleaned_df_with_zero)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error in alternative analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ae32f",
   "metadata": {},
   "source": [
    "### FIRE WEATHER INDEX (FWI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae3da2",
   "metadata": {},
   "source": [
    "### fwi.csv\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_fwi\" / \"chart_fwi\" (i.e., Fire Weather Index (FWI), January - December)\n",
    "#### 2.) \"plot_fwi_d\" / \"chart_fwi_d\" (Fire Weather Index (FWI) Danger Level Distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aee27f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw fire weather index data info:\n",
      "Shape: (53, 2)\n",
      "Columns: ['week', 'pctile_95']\n",
      "Total data points: 53\n",
      "Week range: 1 - 53\n",
      "FWI range: 55.25 - 137.12\n",
      "Data preview:\n",
      "   week   pctile_95\n",
      "0     1  112.633472\n",
      "1     2  113.747966\n",
      "2     3  115.733023\n",
      "3     4  112.483855\n",
      "4     5  125.499134\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/fwi.csv\n",
      "Weeks covered: 53 weeks\n",
      "Week range: 1 - 53\n",
      "FWI range: 55.25 - 137.12\n",
      "Danger level distribution:\n",
      "  Very low: 0 weeks (0.0%)\n",
      "  Low: 0 weeks (0.0%)\n",
      "  Moderate: 0 weeks (0.0%)\n",
      "  High: 0 weeks (0.0%)\n",
      "  Very high: 0 weeks (0.0%)\n",
      "  Extreme: 53 weeks (100.0%)\n",
      "Peak fire weather month: Feb (max FWI: 137.12)\n",
      "Fire weather index data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (53, 4)\n",
      "Cleaned data columns: ['week', 'monthName', 'fwi', 'danger']\n",
      "Sample of cleaned data:\n",
      "   week monthName     fwi   danger\n",
      "0     1       Jan  112.63  Extreme\n",
      "1     2       Jan  113.75  Extreme\n",
      "2     3       Jan  115.73  Extreme\n",
      "3     4       Jan  112.48  Extreme\n",
      "4     5       Feb  125.50  Extreme\n",
      "5     6       Feb  128.72  Extreme\n",
      "6     7       Feb  117.58  Extreme\n",
      "7     8       Feb  114.49  Extreme\n",
      "8     9       Feb  137.12  Extreme\n",
      "9    10       Mar  116.80  Extreme\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 0\n",
      "- Week coverage: 53 weeks\n",
      "- Week range: 1 - 53\n",
      "- FWI range: 55.25 - 137.12\n",
      "\n",
      "Fire Weather Data Summary:\n",
      "- Total weeks: 53\n",
      "- Average FWI: 99.47\n",
      "- Median FWI: 103.56\n",
      "- Standard deviation: 19.53\n",
      "- Highest FWI: 137.12 (Week 9, Feb, Extreme)\n",
      "- Lowest FWI: 55.25 (Week 36, Sep, Extreme)\n",
      "\n",
      "Danger Level Distribution:\n",
      "- Very low: 0 weeks (0.0%)\n",
      "- Low: 0 weeks (0.0%)\n",
      "- Moderate: 0 weeks (0.0%)\n",
      "- High: 0 weeks (0.0%)\n",
      "- Very high: 0 weeks (0.0%)\n",
      "- Extreme: 53 weeks (100.0%)\n",
      "\n",
      "Monthly FWI Statistics:\n",
      "- Jan: Mean 113.65, Range 112.48 - 115.73\n",
      "- Feb: Mean 124.68, Range 114.49 - 137.12\n",
      "- Mar: Mean 115.49, Range 112.23 - 116.91\n",
      "- Apr: Mean 109.97, Range 97.50 - 126.31\n",
      "- May: Mean 107.39, Range 101.98 - 111.52\n",
      "- Jun: Mean 105.59, Range 101.29 - 111.57\n",
      "- Jul: Mean 78.17, Range 70.93 - 85.00\n",
      "- Aug: Mean 64.83, Range 57.06 - 74.43\n",
      "- Sep: Mean 68.01, Range 55.25 - 81.09\n",
      "- Oct: Mean 98.79, Range 97.06 - 99.78\n",
      "- Nov: Mean 102.24, Range 99.19 - 103.56\n",
      "- Dec: Mean 103.30, Range 93.61 - 109.36\n",
      "\n",
      "Data Quality Checks:\n",
      "No data quality issues detected\n",
      "\n",
      "Cleaned data saved to: data/processed/fwi.csv\n",
      "\n",
      " Data structure summary:\n",
      "- Columns: ['week', 'monthName', 'fwi', 'danger']\n",
      "- Time series length: 53 weeks\n",
      "- Data types: {'week': dtype('int64'), 'monthName': dtype('O'), 'fwi': dtype('float64'), 'danger': dtype('O')}\n",
      "- FWI value range: 55.25 - 137.12\n"
     ]
    }
   ],
   "source": [
    "# FIRE WEATHER INDEX - fwi.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_fwi\"/ \"chart_fwi\" (Fire Weather Index (FWI), January - December)\n",
    "# 2.) \"plot_fwi_d\" / \"chart_fwi_d\" (Fire Weather Index (FWI) Danger Level Distribution)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_fwi = pd.read_csv('data/raw/2025-11-mauritania-nouakchott_02-process-output_tabular_nouakchott_fwi.csv') # updatefile path\n",
    "\n",
    "# basic info about raw data\n",
    "print(\"Raw fire weather index data info:\")\n",
    "print(f\"Shape: {raw_df_fwi.shape}\")\n",
    "print(f\"Columns: {list(raw_df_fwi.columns)}\")\n",
    "print(f\"Total data points: {len(raw_df_fwi)}\")\n",
    "\n",
    "# preview key data ranges if available\n",
    "if 'week' in raw_df_fwi.columns:\n",
    "    print(f\"Week range: {raw_df_fwi['week'].min()} - {raw_df_fwi['week'].max()}\")\n",
    "if 'pctile_95' in raw_df_fwi.columns:\n",
    "    print(f\"FWI range: {raw_df_fwi['pctile_95'].min():.2f} - {raw_df_fwi['pctile_95'].max():.2f}\")\n",
    "\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_fwi.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean data using clean_fwi function in clean.py\n",
    "try:\n",
    "    cleaned_df_fwi = clean_fwi('data/raw/2025-11-mauritania-nouakchott_02-process-output_tabular_nouakchott_fwi.csv') # updatefile path\n",
    "    print(\"Fire weather index data cleaned successfully!\")\n",
    "    \n",
    "    # cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_fwi.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_fwi.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_fwi.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_fwi.isnull().sum().sum()}\")\n",
    "    print(f\"- Week coverage: {len(cleaned_df_fwi)} weeks\")\n",
    "    print(f\"- Week range: {cleaned_df_fwi['week'].min()} - {cleaned_df_fwi['week'].max()}\")\n",
    "    print(f\"- FWI range: {cleaned_df_fwi['fwi'].min():.2f} - {cleaned_df_fwi['fwi'].max():.2f}\")\n",
    "    \n",
    "    # fire weather analysis\n",
    "    print(f\"\\nFire Weather Data Summary:\")\n",
    "    \n",
    "    # basic statistics\n",
    "    avg_fwi = cleaned_df_fwi['fwi'].mean()\n",
    "    median_fwi = cleaned_df_fwi['fwi'].median()\n",
    "    std_fwi = cleaned_df_fwi['fwi'].std()\n",
    "    \n",
    "    print(f\"- Total weeks: {len(cleaned_df_fwi)}\")\n",
    "    print(f\"- Average FWI: {avg_fwi:.2f}\")\n",
    "    print(f\"- Median FWI: {median_fwi:.2f}\")\n",
    "    print(f\"- Standard deviation: {std_fwi:.2f}\")\n",
    "    \n",
    "    # ID extremes\n",
    "    peak_week = cleaned_df_fwi.loc[cleaned_df_fwi['fwi'].idxmax()]\n",
    "    lowest_week = cleaned_df_fwi.loc[cleaned_df_fwi['fwi'].idxmin()]\n",
    "    \n",
    "    print(f\"- Highest FWI: {peak_week['fwi']:.2f} (Week {peak_week['week']}, {peak_week['monthName']}, {peak_week['danger']})\")\n",
    "    print(f\"- Lowest FWI: {lowest_week['fwi']:.2f} (Week {lowest_week['week']}, {lowest_week['monthName']}, {lowest_week['danger']})\")\n",
    "    \n",
    "    # danger level distribution\n",
    "    danger_counts = cleaned_df_fwi['danger'].value_counts()\n",
    "    print(f\"\\nDanger Level Distribution:\")\n",
    "    for level in ['Very low', 'Low', 'Moderate', 'High', 'Very high', 'Extreme']:\n",
    "        count = danger_counts.get(level, 0)\n",
    "        percentage = (count / len(cleaned_df_fwi)) * 100\n",
    "        print(f\"- {level}: {count} weeks ({percentage:.1f}%)\")\n",
    "    \n",
    "    # monthly statistics\n",
    "    monthly_stats = cleaned_df_fwi.groupby('monthName')['fwi'].agg(['mean', 'max', 'min']).round(2)\n",
    "    \n",
    "    print(f\"\\nMonthly FWI Statistics:\")\n",
    "    for month in ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']:\n",
    "        if month in monthly_stats.index:\n",
    "            stats = monthly_stats.loc[month]\n",
    "            print(f\"- {month}: Mean {stats['mean']:.2f}, Range {stats['min']:.2f} - {stats['max']:.2f}\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values in key columns\n",
    "    missing_week = cleaned_df_fwi['week'].isna().sum()\n",
    "    missing_fwi = cleaned_df_fwi['fwi'].isna().sum()\n",
    "    missing_month = cleaned_df_fwi['monthName'].isna().sum()\n",
    "    missing_danger = cleaned_df_fwi['danger'].isna().sum()\n",
    "    \n",
    "    if missing_week > 0:\n",
    "        print(f\"Missing week values: {missing_week}\")\n",
    "        quality_issues += 1\n",
    "    if missing_fwi > 0:\n",
    "        print(f\"Missing FWI values: {missing_fwi}\")\n",
    "        quality_issues += 1\n",
    "    if missing_month > 0:\n",
    "        print(f\"Missing month values: {missing_month}\")\n",
    "        quality_issues += 1\n",
    "    if missing_danger > 0:\n",
    "        print(f\"Missing danger values: {missing_danger}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for impossible values\n",
    "    negative_fwi = (cleaned_df_fwi['fwi'] < 0).sum()\n",
    "    if negative_fwi > 0:\n",
    "        print(f\"Negative FWI values: {negative_fwi}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for week sequence\n",
    "    expected_weeks = set(range(1, 54))  # (Note: 53 weeks in a year) \n",
    "    actual_weeks = set(cleaned_df_fwi['week'].unique())\n",
    "    missing_weeks = expected_weeks - actual_weeks\n",
    "    if len(missing_weeks) > 0:\n",
    "        print(f\"Missing weeks: {sorted(list(missing_weeks))}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate weeks\n",
    "    duplicates = cleaned_df_fwi['week'].duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Duplicate week entries: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for valid danger categories\n",
    "    valid_dangers = {'Very low', 'Low', 'Moderate', 'High', 'Very high', 'Extreme', 'Unknown'}\n",
    "    invalid_dangers = set(cleaned_df_fwi['danger'].unique()) - valid_dangers\n",
    "    if len(invalid_dangers) > 0:\n",
    "        print(f\"Invalid danger categories: {invalid_dangers}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"No data quality issues detected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error cleaning fire weather index data: {e}\")\n",
    "    print(\"Check that the FWI CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: week, pctile_95\")\n",
    "\n",
    "# save confirmation and next steps\n",
    "if 'cleaned_df_fwi' in locals():\n",
    "    print(f\"\\nCleaned data saved to: data/processed/fwi.csv\")\n",
    "    \n",
    "    # preview of data structure\n",
    "    print(f\"\\n Data structure summary:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_fwi.columns)}\")\n",
    "    print(f\"- Time series length: {len(cleaned_df_fwi)} weeks\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_fwi.dtypes)}\")\n",
    "    print(f\"- FWI value range: {cleaned_df_fwi['fwi'].min():.2f} - {cleaned_df_fwi['fwi'].max():.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No cleaned fire weather data available\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure FWI CSV file exists in data/raw/\")\n",
    "    print(\"   2. Check file has correct columns: week, pctile_95\")\n",
    "    print(\"   3. Verify week numbers are sequential (1-53)\")\n",
    "    print(\"   4. Check that FWI values are numeric and non-negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
